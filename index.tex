% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\newcommand{\bs}{\symbf}
\newcommand{\mb}{\symbf}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Unif}{\text{Unif}}
\newcommand{\se}{\textsf{se}}
\newcommand{\au}{\underline{a}}
\newcommand{\du}{\underline{d}}
\newcommand{\Au}{\underline{A}}
\newcommand{\Du}{\underline{D}}
\newcommand{\xu}{\underline{x}}
\newcommand{\Xu}{\underline{X}}
\newcommand{\Yu}{\underline{Y}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\U}{\mb{U}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\bbL}{\mathbb{L}}
\renewcommand{\u}{\mb{u}}
\renewcommand{\v}{\mb{v}}
\newcommand{\M}{\mb{M}}
\newcommand{\X}{\mb{X}}
\newcommand{\Xmat}{\mathbb{X}}
\newcommand{\bfx}{\mb{x}}
\newcommand{\y}{\mb{y}}
\newcommand{\bfbeta}{\mb{\beta}}
\renewcommand{\b}{\symbf{\beta}}
\newcommand{\e}{\bs{\epsilon}}
\newcommand{\bhat}{\widehat{\mb{\beta}}}
\newcommand{\XX}{\Xmat'\Xmat}
\newcommand{\XXinv}{\left(\XX\right)^{-1}}
\newcommand{\hatsig}{\hat{\sigma}^2}
\newcommand{\red}[1]{\textcolor{red!60}{#1}}
\newcommand{\indianred}[1]{\textcolor{indianred}{#1}}
\newcommand{\blue}[1]{\textcolor{blue!60}{#1}}
\newcommand{\dblue}[1]{\textcolor{dodgerblue}{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\inprob}{\overset{p}{\to}}
\newcommand{\indist}{\overset{d}{\to}}
\newcommand{\eframe}{\end{frame}}
\newcommand{\bframe}{\begin{frame}}
\newcommand{\R}{\textsf{\textbf{R}}}
\newcommand{\Rst}{\textsf{\textbf{RStudio}}}
\newcommand{\rfun}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\rpack}[1]{\textbf{#1}}
\newcommand{\rexpr}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\filename}[1]{\texttt{\color{blue}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\renewcommand*{\proofname}{Proof}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={STAT 210 Section Notes},
  pdfauthor={Zad Chin \& Jarell Cheong},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{STAT 210 Section Notes}
\author{Zad Chin \& Jarell Cheong}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, interior hidden, borderline west={3pt}{0pt}{shadecolor}, enhanced, breakable, sharp corners, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\includegraphics{./assets/img/stat210-logo.png}

This website contains section notes for STAT 210: Probability I, a
graduate level probability course at Harvard University taught by
Professor \href{mailto:blitz@g.harvard.edu}{Joe Blitzstein}. These notes
are created by \href{mailto:zadchin@college.harvard.edu}{Zad Chin} and
\href{mailto:jarellcheong@college.harvard.edu}{Jarell Cheong Tze Wen}.

\hypertarget{logistics}{%
\section*{Logistics}\label{logistics}}
\addcontentsline{toc}{section}{Logistics}

\markright{Logistics}

\begin{itemize}
\tightlist
\item
  Sections: 12:00 - 1:15 pm on Fridays at SC112
\item
  Office Hours: 1:30 - 2:45pm on Fridays at SC222
\end{itemize}

In section, we will only briefly go through the results from class,
focusing more on pencil problems that will strengthen your understanding
about those results. Section notes will be posted here before Friday
(latest by Thursday night), and section notes with solutions will be
posted after section (on Friday night). Please note that the solution to
the pencil problem is not the official solution but more of the result
of discussion with peers. If you find any error or mistakes, please
report them on Github. Furthermore, feel free to upload the pencil
problem you wish to be discussed in the following section at the link
\href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

In office hours, we will mostly discuss concepts or problems on the
presently assigned homework. Besides section and office hours, please
feel free to ask questions on Ed, or email Joe or us as well.

\hypertarget{notes}{%
\section*{Notes}\label{notes}}
\addcontentsline{toc}{section}{Notes}

\markright{Notes}

These section notes will be presented as an online book, and the source
for this book at \url{https://zadchin.github.io/STAT210_Section/}. Any
typos or errors can be reported at
\url{https://github.com/zadchin/STAT210_Section/issues}. Thanks for
reading.

This is a Quarto book. To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\hypertarget{acknowledgement}{%
\section*{Acknowledgement}\label{acknowledgement}}
\addcontentsline{toc}{section}{Acknowledgement}

\markright{Acknowledgement}

We extend our profound gratitude to the Department of Statistics at
Harvard University, with a special acknowledgment to Professor Joe
Blitzstein for his invaluable insights and knowledge-sharing.

Our appreciation also goes to our peers from STAT 210 in Fall 2022,
whose contributions to discussions on the textbook problems have been
invaluable.

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-1}{%
\chapter*{Section 1}\label{section-1}}
\addcontentsline{toc}{chapter}{Section 1}

\markboth{Section 1}{Section 1}

Last Updated: 16 Sept 2023

Date: 15 Sept 2023

\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  \(\sigma\)-algebras
\item
  Borel \(\sigma\)-algebras
\item
  Probability measures
\item
  Random variables and random vectors
\item
  Limits of events
\item
  Independence
\item
  Uniqueness and \(\pi-\lambda\)
\item
  Additional Questions
\end{itemize}

\hypertarget{sigma-algebras}{%
\section*{\texorpdfstring{\(\sigma\)-algebras}{\textbackslash sigma-algebras}}\label{sigma-algebras}}
\addcontentsline{toc}{section}{\(\sigma\)-algebras}

\markright{\(\sigma\)-algebras}

In the STAT 210 textbook, we defined a \(\sigma\)-algebra as follows:

\emph{Definition 2.2.1 from textbook}

\leavevmode\vadjust pre{\hypertarget{def-sigma-algebra}{}}%
\begin{definition}[]\label{def-sigma-algebra}

A \(\sigma\)-algebra on \(\Omega\) is a collection \(\mathcal{F}\) of
subsets of \(\Omega\) such that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\emptyset \in \mathcal{F}\).
\item
  If \(A \in \mathcal{F}\), then \(A^c \in \mathcal{F}\).
\item
  If \(A_1, A_2, \cdots \in \mathcal{F}\), then
  \(\cup_{j=1}^{\infty} A_j \in \mathcal{F}\).
\end{enumerate}

\end{definition}

\hypertarget{pencil-2.2.5}{%
\subsection*{✏️ Pencil 2.2.5}\label{pencil-2.2.5}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.2.5}

Show that \(\sigma\)-algebra is automatically closed under countable
intersections, i.e.~if \(A_1, A_2, \cdots\) are in the
\(\sigma\)-algebra \(\mathcal{F}\), then
\(\cap_{j=1}^{\infty} A_j \in \mathcal{F}\). Then, show that the
requirement \(\emptyset \in \mathcal{F}\) can be replaced by the
requirements \(\mathcal{F} \neq \emptyset\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

If \(A_1, A_2, \cdots \in \mathcal{F}\), by De Morgan's laws,
\(\bigcap_{j=1}^{\infty} A_j = \left(\bigcup_{j=1}^{\infty} A_j^c \right)^c \in F\)
since each \(A_j^c \in \mathcal{F}\), which implies that
\(\bigcup_{j=1}^{\infty} A_j^c \in \mathcal{F}\), which further implies
that \(\left(\bigcup_{j=1}^{\infty} A_j^c \right)^c \in F\).

Also note that if \(\mathcal{F} \notin \emptyset, \mathcal{F}\) contains
some element of A. Then \(A \cap A^c = \emptyset \in \mathcal{F}\).

\end{minipage}%
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, colback=white, arc=.35mm, bottomtitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Tip: De Morgan Laws}, rightrule=.15mm, coltitle=black, breakable, opacityback=0, opacitybacktitle=0.6]

\[\left(\bigcup_{n=1}^{\infty} A_n \right)^c = \bigcap_{n=1}^{\infty} A_n ^c.\]

\[\left(\bigcap_{n=1}^{\infty} A_n \right)^c = \bigcup_{n=1}^{\infty} A_n ^c.\]

\end{tcolorbox}

\hypertarget{pencil-2.2.6}{%
\subsection*{✏️ Pencil 2.2.6}\label{pencil-2.2.6}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.2.6}

Check that an intersection of \(\sigma-\)algebras, even uncountably
many, is \(\sigma\)-algebra. Give a simple example showing that a union
of \(\sigma\)-algebras may not be a \(\sigma\)-algebra.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(\{\mathcal{F}_i\}_{i\in I}\) be \(\sigma\)-algebras, and consider
\(\mathcal{F}' = \cup_{i\in I} \mathcal{F}_i\). Since each
\(\mathcal{F}_i contains \emptyset\), \(\emptyset \in \mathcal{F}'\). If
\(A\in \mathcal{F}'\), \(A\in \mathcal{F}_i \forall i\in I\), which
implies that \(A^c \in \mathcal{F}_i \forall i \in I\), so is
\(A^c \in \mathcal{F}'\). If \(A_1, A_2, \cdots, \in \mathcal{F}'\),
\(\bigcup_{j=1}^{\infty} A_j \in \mathcal{F}_i \forall i\in I\), which
implies that \(\bigcup_{j=1}^{\infty} A_j \in \mathcal{F}'\).

Let \(\mathcal{F_1} = \{\emptyset, \{a\}, \{b,c\}, \{a, b, c\}\}\) and
\(\mathcal{F_2} = \{\emptyset, \{a,b\}, \{c\}, \{a, b, c\}\}\), note
that \(\mathcal{F_1} \cup \mathcal{F_2}\) contains \(\{a,b\}\) and
\(\{b,c\}\), but not their intersection.

\end{minipage}%
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-important-color}{\faExclamation}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

As noted in the textbook as well as Hazard 2.2.2, a \(\sigma\)-algebra
\(\mathcal{F}\) is a collection of sets, and an element of
\(\mathcal{F}\) is a subset of \(\omega\). An intersection of
\(\sigma\)-algebras is very different from an intersection of events!

\end{minipage}%
\end{tcolorbox}

\hypertarget{borel-sigma-algebras}{%
\section*{\texorpdfstring{Borel
\(\sigma\)-algebras}{Borel \textbackslash sigma-algebras}}\label{borel-sigma-algebras}}
\addcontentsline{toc}{section}{Borel \(\sigma\)-algebras}

\markright{Borel \(\sigma\)-algebras}

\emph{Definition 2.3.1 from textbook}

\leavevmode\vadjust pre{\hypertarget{def-borel-sigma-algebra}{}}%
\begin{definition}[]\label{def-borel-sigma-algebra}

The Borel \(\sigma\)-algebra \(\mathcal{B}\) on \(\mathbb{R}\) is
defined to be the \(\sigma\)-algebra generated by all open intervals
\((a,b)\) with \(a,b\in \mathbb{R}\). A \emph{Borel set} is a set in the
Borel \(\sigma\)-algebra.

\end{definition}

\hypertarget{pencil-2.3.2}{%
\subsection*{✏️ Pencil 2.3.2}\label{pencil-2.3.2}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.3.2}

Show that the Borel \(\sigma\)-algebra is the same if we use closed
intervals \([a,b]\) instead of open intervals to generate it, and that
it is also the same if we use ``semi-inifinite'' intervals
\((-\infty,b)\) to generate it.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\([a,b] = ((-\infty, a) \cup (b,\infty))^c\in \mathcal{B}. (-\infty, b]=(b, \infty)^c \in \mathcal{B}\).
Furhtermore, since \(\mathbb{Q}\) is dense in \(\mathbb{R}\), there
always exists a sequence of rationals that converge to any real number,
so \((-\infty, b]\) for \(b\in \mathbb{R}\) can be obtained from the
union \((-\infty, b_n) \forall b_n \in \mathbb{Q}\), and letting
\(n \rightarrow \infty\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{probability-measure}{%
\section*{Probability Measure}\label{probability-measure}}
\addcontentsline{toc}{section}{Probability Measure}

\markright{Probability Measure}

\emph{Section 2.5 from textbook}

\leavevmode\vadjust pre{\hypertarget{def-probability}{}}%
\begin{definition}[]\label{def-probability}

A \emph{probability space} \((\Omega, \mathcal{F}, P)\) consists of a
measurable space \((\Omega, \mathcal{F})\) and a map (the probability
function) \(P\) satisfying the following axioms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(P(\emptyset) = 0\), \(P(\Omega) = 1\).
\item
  \(\forall A \in \mathcal{F}\), \(P(A) \geq 0\).
\item
  If \(\{A_i\}^{\infty}_{i=1}\) is a collection of mutually disjoint
  sets in \(\mathcal{F}\), i.e.~\(\forall i \neq j\),
  \(A_i \cap A_j = \emptyset\), then
  \[P\left( \bigcup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty} P(A_i). \]
  \textbackslash end\{itemize\} This property is known as
  \(\sigma\)-additivity.
\end{enumerate}

\end{definition}

\hypertarget{pencil-2.5.1}{%
\subsection*{✏️ Pencil 2.5.1}\label{pencil-2.5.1}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.5.1}

Show that in the first axiom, the condition \(P(\emptyset) = 0\) can be
eliminated.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By the first axiom, \(P(\sigma)=1\), and by the second axiom, we find
that \(P(\sigma)=P(\sigma \cup \emptyset) = P(\sigma)+ P(\emptyset)\)
since each \(\sigma\) and \(\emptyset\) are disjoint by definition,
\(P(\emptyset)=0\) follows.

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-1}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

Show that \(P\) is subadditive, i.e.~if \(\{A_i\}^{\infty}_{i=1}\) is
\textit{any} collection of sets in \(\mathcal{F}\), then
\[P\left( \bigcup_{i=1}^{\infty} A_i\right) \leq  \sum_{i=1}^{\infty} P(A_i).\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let us construct a new collection of subsets of \(\Omega\), denoted by
\(\{B_i\}_{i=1}^{\infty}\), where
\[B_1 = A_1 \text{  and  } B_i = A_i \cap \Big (\bigcup_{j=1}^{i-1}A_j  \Big )^c, i \geq 2 \]

We observe that, \(B_i \in \mathcal{F}\), \(\forall i\geq 1\). Moreover,
\(B_i\)'s are disjoint and satisfies
\(\bigcup_{i=1}^{\infty} A_i = \bigcup_{i=1}^{\infty} B_i\). This
technique of constructing disjoint sets that preserve unions from a
general list of sets is called disjointification. We further note that
\(B_i \subseteq A_i\), \(\forall i\geq 1\), implying
\(P(B_i) \leq P(A_i)\), \(\forall i\geq 1\). Using the
\(\sigma\)-additivity of probability we get
\[P \left(\bigcup_{i=1}^{\infty} A_i\right) = P\left(\bigcup_{i=1}^{\infty} B_i\right) = \sum_{i=1}^{\infty}P(B_i) \leq \sum_{i=1}^{\infty}P(A_i).\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2}{%
\subsection*{✏️ Section Problem 2}\label{section-problem-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2}

Let \(A_1,A_2,...\) be a sequence of sets in \(\mathcal{F}\) such that
\(A_n\uparrow A\) as \(n\to \infty\). Show that,
\(P(A_n) \xrightarrow{n \to \infty} P(A)\). Show that the result also
holds for \(A_n \downarrow A\). Does the result hold for any measure?

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

The notation \(A_n \uparrow A\) means that we have a list of increasing
sets \$A\_1 \subseteq A\_2 \subseteq \ldots{} \$ with the limit being
equal to their union, i.e.~\(A = \bigcup_{i=1}^{\infty}A_i\) \$
(\in \mathcal{F})\$. Let us construct \(\{B_i\}_{i=1}^{\infty}\) by
disjointifying \(\{A_i\}_{i=1}^{\infty}\), as part (i). We observe that,
\[P(A_n) = P(\bigcup_{i=1}^{n} A_i) = P(\bigcup_{i=1}^{n} B_i) = \sum_{i=1}^{n}P(B_i) \xrightarrow{n\to \infty} \sum_{i=1}^{\infty} P(B_i) = P(\bigcup_{i=1}^{\infty} B_i) = P(A)\]
This property is also known as `continuity from below'.

Analogously, \(A_n \downarrow A\) means we have a list of decreasing
sets \$A\_1 \supseteq A\_2 \supseteq \ldots{} \$ with the limit being
equal to their intersection, i.e.~\(A = \bigcap_{i=1}^{\infty}A_i\)
\((\in \mathcal{F})\). For this case, let \(C_i = A_1 \backslash A_i\),
\(i \geq 1\). Then
\(C_n \uparrow A_1 \backslash \bigcap_{i=1}^{\infty}A_i = A_1 \backslash A\).
Applying the previous result, we get
\[P(C_n) \xrightarrow{n\to \infty} P(A_1\backslash A) = P(A_1) - P(A) \]
We complete the proof by noting that \(P(C_n) = P(A_1) - P(A_n)\). This
property is also known as `continuity from above'. Let \(m\) be a
general measure. Following similar steps as above, one can prove that
\(m\) is continuous from below. However, \(m\) is not continuous from
above, in general. As a counterexample, suppose
\(\Omega = \mathbb{R}, \mathcal{F}=\mathcal{B}\) (the Borel
\(\sigma\)-algebra on \(\mathbb{R}\)) and \(m\) is the Lebesgue measure
on \(\mathbb{R}\). Let \(A_n = [n,\infty)\), \(n\geq 1\). Here
\(m(A_n) = \infty\), \(\forall n\geq 1\), but
\(A_n \downarrow \emptyset\) and \(m(\emptyset) = 0\). Notice that, the
continuity property holds for a sequence of sets \(A_n \downarrow A\)
and a general measure \(m\) if \(m(A_k)\) is finite for some
\(k \geq 1\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{random-variables-and-random-vectors}{%
\section*{Random Variables and Random
Vectors}\label{random-variables-and-random-vectors}}
\addcontentsline{toc}{section}{Random Variables and Random Vectors}

\markright{Random Variables and Random Vectors}

\emph{Definition 2.6.1 from textbook}

\leavevmode\vadjust pre{\hypertarget{def-random-variable}{}}%
\begin{definition}[]\label{def-random-variable}

A \emph{random variable} is a measurable function \(X\) from \(\sigma\)
to \(\mathbb{R}\), where ``measurable'' means that the preimage
\(X^{-1}(\mathcal{B}) = \{\omega \in \sigma : X(\omega) \in \mathcal{B}\}\)
is in \(\mathcal{F}\) for all Borel sets \(\mathcal{B}\).

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-random-vector}{}}%
\begin{definition}[]\label{def-random-vector}

A \emph{random vector} in \(\mathbb{R}^n\) is a measurable function
\(\mathbf{X}\) from \(\sigma\) into \(\mathbb{R}^n\), where
``measurable'' means that the preimage
\(\mathbf{X}^{-1} (\mathcal{B}) \equiv \{\omega \in \sigma: \mathbf{X}(\omega) \in \mathcal{B}\}\)
is in \(\mathcal{F}\) for all Borel sets \(\mathcal{B}\) in
\(\mathbb{R}^n\).

\end{definition}

\hypertarget{pencil-2.6.2}{%
\subsection*{✏️ Pencil 2.6.2}\label{pencil-2.6.2}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.6.2}

Show that if \(X\) is a random variable, then so is \(g(X)\) for any
measurable function \(g\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\(X\) is a random variable. By definiton,
\(X^{-1}(\mathcal{B}) \in \mathcal{F}\). \(g\) is measurable, so
\(g^{-1}(\mathcal{B})\) is Borel. Thus,
\((gX)^{-1}(\mathcal{B}) = X^{-1}(g^{-1}(\mathcal{B})) \in \mathcal{F}\),
so \(g(X)\) is a random variable.

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-2.7.2}{%
\subsection*{✏️ Pencil 2.7.2}\label{pencil-2.7.2}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.7.2}

Let \(F(x,y)\) be a bivariate CDF. Prove that for all \(a_1 \leq b_1\)
and \(a_2 \leq b_2\),
\[F(b_1, b_2)-F(a_1, b_2)-F(b_1,a_2)+F(a_1,a_2) \geq 0.\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By the axiom of probability, observe that: \[
\begin{align*}
F(&b_1, b_2)-F(a_1, b_2)-F(b_1,a_2)+F(a_1,a_2)  \\
&= P(X \leq b_1, Y \leq b_2) - P(X \leq a_1, Y\leq b_2)\\
&-P(X \leq b_1, Y \leq a_2) + P(X \leq a_1, Y\leq a_2)\\
&= P(a \leq X \leq b_1, Y \leq b_2) - P(a_1 \leq X\leq b_1, Y \leq a_2)\\
&= P(a_1 \leq X \leq b_1, a_2 \leq Y \leq b_2)\\
&\geq 0\\
\end{align*}
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{limits-of-events}{%
\section*{Limits of Events}\label{limits-of-events}}
\addcontentsline{toc}{section}{Limits of Events}

\markright{Limits of Events}

\emph{Section 2.8 from textbook}

\leavevmode\vadjust pre{\hypertarget{def-lim}{}}%
\begin{definition}[]\label{def-lim}

Let \(A_1, A_2,...\) be a sequence of events on
\((\Omega, \mathcal{F}, P)\). Define

\[B_n = \bigcup_{m=n}^{\infty} A_m ,\hspace{0.2cm} C_n = \bigcap_{m=n}^{\infty} A_m, \hspace{0.2cm} n\geq 1 .\]

Then, \(C_n \subseteq A_n \subseteq B_n\), \$\forall n\geq 1 \$, and
\(\{B_n\}^{\infty}_{n=1}\) and \(\{C_n\}^{\infty}_{n=1}\) are decreasing
and increasing respectively. Let \(B\) and \(C\) be their respective
limiting sets. Then,

\[B = \bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty} A_m ,\hspace{0.2cm} C = \bigcup_{n=1}^{\infty} \bigcap_{m=n}^{\infty} A_m  \]
We call \(B\) and \(C\), \(\limsup\limits_{n \to \infty}A_n\) and
\(\liminf \limits_{n \to \infty}A_n\), respectively.

\end{definition}

\hypertarget{section-problem-3}{%
\subsection*{✏️ Section Problem 3}\label{section-problem-3}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3}

Show that
\(B = \{\omega \in \Omega : \omega \in A_n \text{ for infinitely many values of } n\}\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

It is useful to think about ``\(\bigcup\)'' as ``there exists''
(\(\exists\)) and ``\(\bigcap\)'' as ``for all'' (\(\forall\)). So, \[
\begin{eqnarray*}
    \omega \in B &\iff& \omega \in \bigcap_{n=1}^{\infty} B_n\\
    & \iff & \omega \in B_n, \hspace{0.2cm}\forall n\geq 1\\
    & \iff & \forall n\geq 1, \hspace{0.2cm} \omega \in \bigcup_{m=n}^{\infty} A_m\\
    & \iff & \forall n\geq 1, \hspace{0.1cm} \exists m_n\geq n \text{ such that } \omega \in A_{m_n}\\
    & \iff & \omega \in A_n \text{ for infinitely many values of } n
    \end{eqnarray*}
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-4}{%
\subsection*{✏️ Section Problem 4}\label{section-problem-4}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 4}

Show that
\(C = \{\omega \in \Omega: \omega \in A_n \text{ for all but finitely many values of } n\}\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

We observe that, \[
    \begin{eqnarray*}
    \omega \in C &\iff& \omega \in \bigcup_{n=1}^{\infty} C_n\\
    & \iff & \exists n\geq 1 \text{ such that } \omega \in C_n \\
    & \iff & \exists n\geq 1 \text{ such that } \omega \in \bigcap_{m=n}^{\infty} A_m\\
    & \iff & \exists n\geq 1 \text{ such that } \forall m\geq n,  \omega \in A_m\\
    & \iff & \omega \in A_n \text{ for all but finitely many values of } n
    \end{eqnarray*}
\] We can also prove this result by using part (i). Notice that
\[ \begin{align*}C^c &= \Big(\bigcup_{n=1}^{\infty} \bigcap_{m=n}^{\infty} A_m \Big)^c \\ &= \bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} A^c_m \\ &= \{\omega \in \Omega : \omega \in A^c_n \text{ for infinitely many values of } n\} \end{align*}\].

Thus,
\[ \begin{align*} C &= \{\omega \in \Omega: \omega \in A^c_n \text{ for finitely many values of } n\}  \\ &= \{\omega \in \Omega: \omega \in A_n \text{ for all but finitely many values of } n\} \end{align*}\]

\end{minipage}%
\end{tcolorbox}

\leavevmode\vadjust pre{\hypertarget{def-limits-to-An}{}}%
\begin{definition}[]\label{def-limits-to-An}

We say that the sequence of events \(\{A_n\}^{\infty}_{n=1}\) ``has a
limit \(A\)'', if \(B = C\) and the common set is equal to \(A\). In
that case, we write \(\lim\limits_{n\to \infty} A_n = A\).

\end{definition}

\hypertarget{section-problem-5}{%
\subsection*{✏️ Section Problem 5}\label{section-problem-5}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 5}

Assume the above is the case. Show that \(A \in \mathcal{F}.\)

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Since \(\mathcal{F}\) is closed under countable unions,
\(B_n \in \mathcal{F}\) \(\forall n \geq 1\). Again, since
\(\mathcal{F}\) is closed under countable intersections,
\(\bigcap_{n=1}^{\infty} B_n \in \mathcal{F}\). So,
\(A=B \in \mathcal{F}\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-6}{%
\subsection*{✏️ Section Problem 6}\label{section-problem-6}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 6}

Show that \(\lim\limits_{n \to \infty} P(A_n) = P(A)\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, toptitle=1mm, left=2mm, titlerule=0mm, bottomrule=.15mm, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, colback=white, arc=.35mm, bottomtitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Tip: A useful inequality}, rightrule=.15mm, coltitle=black, breakable, opacityback=0, opacitybacktitle=0.6]

\[P(\liminf\limits_{n \to \infty}A_n) \leq \liminf\limits_{n \to \infty}P(A_n) \leq \limsup\limits_{n \to \infty}P(A_n) \leq P(\limsup\limits_{n \to \infty}A_n).\]

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Note that, the result is immediate if we can prove (1). We shall now
show that (1) holds for a general sequence of events
\(\{A_n\}_{n=1}^{\infty}\).\textbackslash{} We know that (from Problem
2)
\[P(\liminf\limits_{n\to \infty} A_n) =  \lim \limits_{n\to \infty} P(C_n)\]
Note that \(C_n \subseteq A_n\), \(\forall n \geq 1\), which implies
\(P(C_n) \leq P(A_n)\). Hence we have,
\[\lim \limits_{n\to \infty} P(C_n) = \liminf \limits_{n\to \infty} P(C_n) \leq \liminf\limits_{n\to \infty} P(A_n) \]
Similarly, we have
\[P(\limsup\limits_{n\to \infty} A_n) =  \lim \limits_{n\to \infty} P(B_n) \]
Note that \(B_n \supseteq A_n\), \(\forall n \geq 1\), which implies
\(P(B_n) \geq P(A_n)\). Hence we have,
\[\lim \limits_{n\to \infty} P(B_n)=\limsup \limits_{n\to \infty} P(B_n) \geq \limsup\limits_{n\to \infty} P(A_n) \]
Finally, it is always true that
\[\liminf\limits_{n\to \infty} P(A_n) \leq \limsup\limits_{n\to \infty} P(A_n)\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{independence}{%
\section*{Independence}\label{independence}}
\addcontentsline{toc}{section}{Independence}

\markright{Independence}

\leavevmode\vadjust pre{\hypertarget{def-indepence-RV}{}}%
\begin{definition}[]\label{def-indepence-RV}

Two random variables \(X_1,X_2\) are \emph{independent} if the event
\(X_1\in B_1\) is independent of the event \(X_2\in B_2\) for all Borel
\(B_1, B_2\)

\end{definition}

As remarked in Remark 2.9.12, it is helpful and simple to think directly
about distributions and preimages. In set operations, if \(f\) is a
function from a set \(S\) into a set \(T\), and
\(B_{\alpha} \subseteq T\) for all \(\alpha\) in some index set \(A\),
we then have the following convenient properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \$f\^{}\{-1\}(\cup\emph{\alpha B}\{\alpha\}) =
  \cup\emph{\alpha f\^{}\{-1\} (B}\alpha)
\item
  \$f\^{}\{-1\}(\cap\emph{\alpha B}\{\alpha\}) =
  \cap\emph{\alpha f\^{}\{-1\} (B}\alpha)
\item
  \$f\^{}\{-1\} (B\_\alpha\^{}C)
  =(f\textsuperscript{\{-1\}(B\_\alpha))}C
\end{enumerate}

\hypertarget{pencil-2.9.13}{%
\subsection*{✏️ Pencil 2.9.13}\label{pencil-2.9.13}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.9.13}

Verify the above properties of preimages

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose \(x\in f^{-1}(\cup_\alpha B_{\alpha})\), then note that
  \(f(x) \in \cup_\alpha B_\alpha\), which implies that
  \(f(x) \in B_\alpha\) for some \(\alpha\). Thus, \$x \in f\^{}\{-1\}
  (B\_\alpha) for some \(\alpha\), which implies
  \(x \in \cup_\alpha f^{-1} (B_\alpha)\).
\item
  Suppose \(x\in f^{-1}(\cap_\alpha B_{\alpha})\). Analogously, by
  replacing ``some'' with ``all'',
  \(x\in \cap_\alpha f^{-1} (B_\alpha)\)
\item
  Suppose \(x\in f^{-1} (B_\alpha^C)\), then \(f(x) \in B_\alpha^C\),
  which is equivalent to \(f(x)\notin B_\alpha\). Thus,
  \(x\notin f^{-1}(B_\alpha)\), which implies that
  \(x\in (f^{-1}(B_\alpha))^C\)
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-2.9.15}{%
\subsection*{✏️ Pencil 2.9.15}\label{pencil-2.9.15}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.9.15}

If \(\mathbf{X}_1, \mathbf{X}_2 , \mathbf{X}_3\) are random vectors
(possibly in different dimensions) with
\(\mathbf{X}_1 \perp\!\!\!\perp \mathbf{X}_2\) and
\((\mathbf{X}_1 , \mathbf{X}_2) \perp\!\!\!\perp \mathbf{X}_3\), then
\(\mathbf{X}_1, \mathbf{X}_2 , \mathbf{X}_3\) are independent.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\[
\begin{align*}
P(\mathbf{X}_1 \in A, \mathbf{X}_2 \in B, \mathbf{X}_3 \in C) 
&= P(\mathbf{X}_1, \mathbf{X}_2 \in A \times B, \mathbf{X}_3 \in C)\\
&= P(\mathbf{X}_1, \mathbf{X}_2 \in A \times B)P(\mathbf{X}_3 \in C)\\
&= P(\mathbf{X}_1 \in A, \mathbf{X}_2 \in B)P(\mathbf{X}_3 \in C)\\
&= P(\mathbf{X}_1 \in A)P(\mathbf{X}_2 \in B)P(\mathbf{X}_3 \in C)\\
\end{align*}
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{uniqueness-and-pi-lambda}{%
\section*{\texorpdfstring{Uniqueness and
\(\pi-\lambda\)}{Uniqueness and \textbackslash pi-\textbackslash lambda}}\label{uniqueness-and-pi-lambda}}
\addcontentsline{toc}{section}{Uniqueness and \(\pi-\lambda\)}

\markright{Uniqueness and \(\pi-\lambda\)}

\leavevmode\vadjust pre{\hypertarget{def-indepence-RV}{}}%
\begin{definition}[]\label{def-indepence-RV}

A collection \(\mathcal{A}\) of subsets of \(\Omega\) is called a
\emph{\(\pi-\)system} if \(A\cap B\in \mathcal{A}\) for all
\(A, B\in \mathcal{A}\) The collection \(\mathcal{L}\) of subsets of
\(\Omega\) is called a \emph{\(\lambda-system\)} if the following
conditions hold:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\Omega \in \mathcal{L}\)
\item
  (Closure under proper difference) If \(A, B \in \mathcal{L}\) with
  \(A \subseteq B\), then \$B ~A \in \mathcal{L}
\item
  (Closure under increasing unions) If
  \(A_1 \subseteq A_2 \subseteq \cdots\) are in \(\mathcal{L}\), then
  \(\bigcup_{i=1}^\infty A_i \in \mathcal{L}\)
\end{enumerate}

\end{definition}

\hypertarget{pencil-2.10.2}{%
\subsection*{✏️ Pencil 2.10.2}\label{pencil-2.10.2}}
\addcontentsline{toc}{subsection}{✏️ Pencil 2.10.2}

Show that if \(\mathcal{A}\) is both a \(\pi\)-system and a
\(\lambda\)-system, then it is a \(\sigma\)-algebra.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\(\Omega \in \mathcal{A}\), so for any \(A\in \mathcal{A}\),
\(A \subseteq \Omega\). This implies that
\(\Omega-A = A^c \in \mathcal{A}\). Trivially, we also observe that
\(\mathcal{A}\neq \emptyset\). Finally, if
\(A_1, A_2, \cdots \in \mathcal{A}\), then
\(A_1 \cap A_2 \cap \cdots \subseteq \cdots \subseteq A \in \mathcal{A}\),
so their union \(\bigcup_{i=1}^\infty A_i \in \mathcal{A}\)

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week}{%
\section*{Next Week}\label{next-week}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  Discuss more \(\pi-\lambda\)
\item
  Reasoning by representation
\end{itemize}

\includegraphics{./assets/img/meme/meme1.png}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-2}{%
\chapter*{Section 2}\label{section-2}}
\addcontentsline{toc}{chapter}{Section 2}

\markboth{Section 2}{Section 2}

Last Updated: 21 Sept 2023

Date: 22 Sept 2023

\hypertarget{introduction-1}{%
\section*{Introduction}\label{introduction-1}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  Reasoning by Representation
\item
  Derivation path of distribution representations
\item
  Poisson Processes
\end{itemize}

\hypertarget{reasoning-by-representation}{%
\section*{Reasoning by
Representation}\label{reasoning-by-representation}}
\addcontentsline{toc}{section}{Reasoning by Representation}

\markright{Reasoning by Representation}

Some of the important terms and definitons of Chapter 3 is as follow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Bernoulli}: A random variable \(Y\) has the \emph{Bernoulli
  distribution} with parameter \(p\), denoted \(Y\sim Bern(p)\), if
  \(P(Y=1)=p\) and \(P(Y=0)=1-p\)
\item
  \textbf{Binomial}: sum of \(n\) iid Bernoulli random variables
\item
  \textbf{Uniform}: \(U = \sum_{i=1}^\infty \frac{B_i}{2^i} \sim\) Unif
  where \(B_i\) iid \(Bern(0.5)\).
\item
  \textbf{PIT sampling}: Let \(F\) be \textit{any} CDF with quantile
  function \(F^{-1}\), let \(U \sim Unif\) then \(F^{-1}(U) \sim F.\)
\item
  \textbf{PIT pivoting}: Let \(F\) be a continuous CDF, and
  \(Y \sim F\), then \(F(Y)\sim Unif.\)
\item
  \textbf{Exponential}: Let \(U\sim\) Unif. Then
  \(X = -\log U \sim \textnormal{Expo}.\)
\item
  \textbf{Gamma}: For a positive integer \(n\),
  \(G_n= X_1+X_2+\cdots + X_n \sim Gamma(n)\), where
  \(X_i \overset{i.i.d}{\sim}\) Expo.
\item
  \textbf{Laplace} : \(L \sim \textnormal{Laplace}\) if \(L \sim SX\)
  where \(S\) is random sign and \(X\sim Expo.\)
\item
  \textbf{Weibull}: \(W = X^\beta \sim\) Wei(\(\beta\)) where
  \(X \sim Expo\) and \(\beta >0.\)
\item
  \textbf{Beta}: \(B = \frac{G_a}{G_a+G_b} \sim Beta(a,b)\) where
  \(G_a \sim Gamma(a)\), \(G_b\sim Gamma(b)\) and
  \(G_a \perp\!\!\!\!\perp G_b\); note that
  \(U^{1/\alpha} \sim Beta(\alpha,1).\)
\item
  \textbf{Beta-Gamma}: Let
  \(G_a \sim Gamma(a) \perp\!\!\!\!\perp G_b\sim Gamma(b)\) ,
  \(B = \frac{G_a}{G_a+G_b}\) and \(T = G_a + G_b \sim Gamma(a+b).\)
  Then \(T \perp\!\!\!\!\perp B.\)
\item
  \textbf{Chi-squared}: \(G \sim \chi_n^2\) if
  \(G\sim 2Gamma\left(\frac{n}{2}\right).\)
\item
  \textbf{Normal}: \(Z = SX \sim \mathcal{N}(0,1)\) where \(S\) is
  random sign and \(X\sim \chi_1.\)
\item
  \textbf{Box-Muller}: \(U_1,U_2 \overset{i.i.d}{\sim}\)Unif. Then
  \(\sqrt{-2\ln U_2}\cos{(2\pi U_1)}, \sqrt{-2\ln U_2}\sin{(2\pi U_1)} \overset{i.i.d}{\sim} \mathcal{N}(0,1)\)
\item
  \textbf{Student \(t\)-distribution}:
  \(T = \frac{Z}{\sqrt{V_n/n}} \sim t_n\) where
  \(Z \sim N(0,1) \perp\!\!\!\!\perp V_n \sim \chi_n^2.\)
\item
  \textbf{Cauchy}:
  \(C = \frac{Z_1}{Z_2} \sim \textnormal{Cauchy} \sim t_1\) where
  \(Z_1,Z_2 \sim_{i.i.d} \mathcal{N}(0,1)\).
\end{enumerate}

For ease of referencing, Eric Zhang have summarized how to move from one
distribution to next in a figure, in which we will discuss in the next
section.

\hypertarget{derivation-path-of-distribution-representations}{%
\section*{Derivation path of distribution
representations:}\label{derivation-path-of-distribution-representations}}
\addcontentsline{toc}{section}{Derivation path of distribution
representations:}

\markright{Derivation path of distribution representations:}

Below is a pitcure showing derivation path of distribution
representations, kindly created by
\href{https://www.ekzhang.com/assets/pdf/Stat_210_Notes.pdf}{Eric
Zhang}:

\includegraphics{./assets/img/derivation.png}

\hypertarget{pencil-3.3.2}{%
\subsection*{✏️ Pencil 3.3.2}\label{pencil-3.3.2}}
\addcontentsline{toc}{subsection}{✏️ Pencil 3.3.2}

Show that if \(U\sim Unif\), then \(1-U \sim Unif\). Also, show that
\(2U-1 \sim SU\) for \(S\) a random sign independent of \(U\) (so
\(2U-1\) is symmetric about 0, while \(U\) is symmetric about \(1/2\);
see Section 3.10 for more information.)

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

If \(U\sim Unif\), \(1-U=1+(0-1)U\) is Uniform between 1 and 0 so
\(1-U \sim Unif\) as well.

\(2U-1 = -1 +(1-(-1))U\) is Uniform between \(-1\) and 1, and SU is also
Uniform between -1 and 1 by definition, so \(2U-1 \sim SU\)

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-3.5.10}{%
\subsection*{✏️ Pencil 3.5.10}\label{pencil-3.5.10}}
\addcontentsline{toc}{subsection}{✏️ Pencil 3.5.10}

Let \(W_1 \sim Wei(\beta_1), W_2 \sim Wei(\beta_2)\). Show that
\((W_1|W_1 \geq 1) \preceq (W_2|W_2 \geq 1)\) iff
\(\beta_1 \leq \beta_2\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\((W_1|W_1 \geq 1) \preceq (W_2|W_2 \geq 1)\) implies that
\(F_1(w) \geq F_2(w)\), which by definition implies that
\(1-exp(-w^{1/\beta_1}) \geq 1-exp(-w^{1/\beta_2})\). This further
implies that \(exp(-w^{1/\beta_2}) \geq exp(-w^{1/\beta_1})\), which
implies that \(-w^{1/\beta_2} \geq -w^{1/\beta_1}\), whcih is similar to
\(w^{1/\beta_1} \geq w^{1/\beta_2}\), which further implies than when
\(w\geq 1\), \(\frac{1}{\beta_1} \geq \frac{1}{\beta_2}\), thus we show
that \(\beta_1 \leq \beta_2\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-3.6.16}{%
\subsection*{✏️ Pencil 3.6.16}\label{pencil-3.6.16}}
\addcontentsline{toc}{subsection}{✏️ Pencil 3.6.16}

Let \(C \sim\) Cauchy and \(U \sim\) Unif. Show that
\[tan(2 \pi U) \sim C\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

C= \(Z_1 / Z_2 = W sin (2\pi U) /W cos (2\pi U) = tan(\pi U)\)

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-3.7.18}{%
\subsection*{✏️ Pencil 3.7.18}\label{pencil-3.7.18}}
\addcontentsline{toc}{subsection}{✏️ Pencil 3.7.18}

Show that if \(C\sim\) Cauchy, \(S\) is a random sign, and \(B\sim\)
Beta(1/2, 1/2), then
\[C+\frac{1}{C} \sim 2S\sqrt{1+C^2}\sim \frac{2S}{\sqrt{B}}\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\[
\begin{align*}
1 + C^2 &= 1 + \frac{Z_1^2}{Z_2^2} \\
&= \frac{Z_2^2 + Z_1^2}{Z_2^2} \\
&= \frac{\chi_1}{\chi_2} \\
&= \frac{G_1}{G_{1/2}} \\
&= \frac{G_{1/2} + G_{1/2}}{G_{1/2}} \\
&= \frac{1}{B}
\end{align*}
\] Thus, \(2S\sqrt{1+C^2} \sim 2S/\sqrt{B}\) follows.

Next, I am lazy to type so ask in office hour!

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-3.9.5}{%
\subsection*{✏️ Pencil 3.9.5}\label{pencil-3.9.5}}
\addcontentsline{toc}{subsection}{✏️ Pencil 3.9.5}

Show that NBin\((r,p)\) PMF is \(P(X=x)=\) \(r+x-1 \choose x\)
\(p^r q^x\), where \(q\equiv 1-p\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Under convolution, we get that
\(P(X=x)=q^x p \times cdots \times q^x p (r)\) times, which can be
checked to be \(r+x-1 \choose x\) \(p^r q^x\)

\end{minipage}%
\end{tcolorbox}

\hypertarget{pencil-3.11.2}{%
\subsection*{✏️ Pencil 3.11.2}\label{pencil-3.11.2}}
\addcontentsline{toc}{subsection}{✏️ Pencil 3.11.2}

Suppose that \(Y_1, \cdots, Y_n \sim Expo\) are i.i.d. Show that the
minimum is also Exponentially distributed, with \(n\) times the rate:
\(Y_{(1)} \sim \frac{1}{n} Expo\) (the rate parameter is defined to be
reciprocal of the scale parameter)

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\[
\begin{align*}
P(min\{Y_1, \cdots, Y_n\}>y)  
&=P(Y_1 > y, \cdots, Y_n >y) \\
&=P(Y_1 > y) \cdots P(Y_n > y) \\
&= e^{-y} \cdots e^{-y} \\
&= e^{-ny} 
\end{align*}
\]

So \(P(min\{Y_1, \cdots, Y_n\}>y) = 1-e^{-ny} \sim n^{-1}Expo\)

\end{minipage}%
\end{tcolorbox}

\hypertarget{poisson-process}{%
\section*{Poisson Process}\label{poisson-process}}
\addcontentsline{toc}{section}{Poisson Process}

\markright{Poisson Process}

From Stat 110 textbook on pg 559:

\leavevmode\vadjust pre{\hypertarget{def-poisson-process}{}}%
\begin{definition}[]\label{def-poisson-process}

A sequence of arrivals in continuous time is a \emph{Poisson process}
with rate \(\lambda\) if the following conditions hold:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The number of arrivals in an interval length \(t\) is distributed
  Pois\((\lambda t)\)
\item
  The numbers of arrivals in disjoint time intervals are independent.
\end{enumerate}

\end{definition}

Recall that in STAT 110, we also learn how to generate 1D Poisson
Process, detailed in pg 560 of the textbook:

\leavevmode\vadjust pre{\hypertarget{thm-poisson-story}{}}%
\begin{theorem}[]\label{thm-poisson-story}

(Generative Story for 1D Poisson process) To generate \(n\) arrivals
from a Poisson process on \((0, \infty)\) with rate \(\lambda\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate \(n\) i.i.d. Expo\((\lambda)\) r.v.s \(X_1, \cdots, X_n\)
\item
  For \(j=1, \cdots, n\), set \(T_j = X_1 + \cdots+X_j\)
\end{enumerate}

Then we can take \(T_1, \cdots, T_n\) to be the arrival times.

\emph{Ref: Story 13.1.2 in STAT 110 textbook}

\end{theorem}

Also note that on Theorem 13.2.1 in STAT 110 textbook:

\leavevmode\vadjust pre{\hypertarget{thm-conditional-counts}{}}%
\begin{theorem}[]\label{thm-conditional-counts}

(Conditional Counts) Let \((N(t):t>0)\) be a Poisson process with rate
\(\lambda\), and \(t_1<t_2\). The conditional distribution of \(N(t_1)\)
given \(N(t_2) = n\) is
\[N(t_1)|N(t_2)=n \sim Bin \left(n, \frac{t_1}{t_2}\right)\]

\end{theorem}

also note that Propositional 13.2.2 in STAT 110 textbook states that In
a Poisson process of rate \(\lambda\), conditional on \(N(t) =1\), the
first arrival time \(T_1\) has the Unif\((0,t)\) distribution.

Some other important concept about Poisson process from STAT 110
textbook include:

\leavevmode\vadjust pre{\hypertarget{thm-conditional-time}{}}%
\begin{theorem}[]\label{thm-conditional-time}

(Conditional times) In a process process of rate\(\lambda\), conditional
on \(N(t)=n\), the joint distribution of the arrival times
\(T_1, \cdots, T_n\) is the same as the joint distribution of the order
statistics of \(n\) i.i.d Unif\((0,t)\) r.v.s.

Also note that we know that the order statistics of Unif(0,1) r.v..s are
Betas, so the conditional distributions of the \(T_j\) are \emph{scaled}
Betas. To get Beta Distribution, we can just divide the \(T_j\) by \(t\)
so that their support is \((0,1)\):
\[t^{-1}T_j | N(t) = n \sim Beta(j, n-j+1)\]

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-poisson-story}{}}%
\begin{theorem}[]\label{thm-poisson-story}

(Generative Story for Poisson process) To generate \(n\) arrivals from a
Poisson process on \((0, t]\) with rate \(\lambda\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate the total number of events in the interval,
  \(N(t) \sim Pois(\lambda t)\)
\item
  Given \(N(t) = n\), generate \(n\) i.i.d Unif\((0,t)\) r.v.s
  \(U_1, \cdots, U_n\)
\item
  For \(j=1, \cdots, n\), set \(T_j = U_{(j)}\)
\end{enumerate}

\emph{Ref: Story 13.2.4 in STAT 110 textbook}

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-superposition}{}}%
\begin{theorem}[]\label{thm-superposition}

(Superposition). Let \((N_1(t): t>0)\) and \((N_2(t): t>0)\) be
independent Poisson process with rates \(\lambda_1\) and \(\lambda_2\)
respectively. Then the combined process \(N(t) = N_1(t) + N_2(t)\) is a
Poisson process with rate \(\lambda_1+\lambda_2\).

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-poisson-story-superposition}{}}%
\begin{theorem}[]\label{thm-poisson-story-superposition}

(Generative Story for superposition) To generate the superposition of
two independent Poisson processes, \((N_1(t): t>0)\) with
rate\(\lambda_1\), and \((N_2(t): t>0)\) with rate \(\lambda_2\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate arrivals from the Poisson process \((N_1(t): t>0)\)
\item
  Generate arrivals from the Poisson process \((N_2(t): t>0)\)
\item
  Superpose the results of steps 1 and 2.
\end{enumerate}

\emph{Ref: Story 13.2.7 in STAT 110 textbook}

\end{theorem}

There is also another generative story for superposition, as highlighted
below:

\leavevmode\vadjust pre{\hypertarget{thm-poisson-story-superposition-2}{}}%
\begin{theorem}[]\label{thm-poisson-story-superposition-2}

(Generative Story for superposition, take 2) To generate the
superposition of two independent Poisson processes, with
rate\(\lambda_1\) and \(\lambda_2\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate i.i.d Expo\((\lambda_1 +\lambda_2)\) r.v.s
  \(X_1, X_2,\cdots\) and let the \(j\)th arrival at time
  \(T_j =X_1+\cdots+X_j\)
\item
  Generate i.i.d r.v.s.
  \(I_1, I_2, \cdots Bern(\lambda_1/(\lambda_1+\lambda_2))\),
  independent of \(X_1, X_2, \cdots\). Let the \(j\)th arrival be type-1
  if \(I_j=1\), and type-2 otherwise.
\end{enumerate}

\emph{Ref: Story 13.2.9 in STAT 110 textbook}

\end{theorem}

Following up from the superposition definition and generative story
above, the two most important theorem arive from this are

\leavevmode\vadjust pre{\hypertarget{thm-superposition-discrete}{}}%
\begin{theorem}[]\label{thm-superposition-discrete}

(Projection of superposition into discrete time) Consider the
superposition \((N(t);t>0)\) of two independent Poisson processes with
rate \(\lambda_1\) and \(\lambda_2\). For \(j=1,2,\cdots\), let \(I_j\)
be the indicator of the \(j\)th event being from the Poisson process
with rate \(\lambda_1\). Then the \(I_j\) are i.i.d
Bern\((\lambda_1/(\lambda_1+\lambda_2))\)

\emph{Ref: Thm 13.2.11 in STAT 110 textbook}

\end{theorem}

Using the result above, we can orive wutg a story that a Gamma mixture
of Poissons is Negative Binomial:

\leavevmode\vadjust pre{\hypertarget{thm-mixture-poisson}{}}%
\begin{theorem}[]\label{thm-mixture-poisson}

(Exponential mixture of Poissons is Geometric). Suppose that
\(X \sim Expo(\lambda)\), and \(Y|X=x \sim Pois(\lambda)\). Then
\(Y\sim Geom(\lambda/(\lambda+1))\)

\emph{Ref: Thm 13.2.12 in STAT 110 textbook}

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-mixture-poisson-negative-binomial}{}}%
\begin{theorem}[]\label{thm-mixture-poisson-negative-binomial}

(Gamma mixture of Poissons is Negative Binomial). Suppose that
\(X \sim Gamma(r, \lambda)\), and \(Y|X=x \sim Pois(x)\). Then
\(Y\sim Nbin(r, \lambda/(\lambda+1))\)

\emph{Ref: Thm 13.2.12 in STAT 110 textbook}

\end{theorem}

\hypertarget{section-discussion-questions}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

\hypertarget{section-problem-1-1}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

Let \(A, B, C\) be i.i.d. Uniform\((0,1)\), which are coefficients of
the following ``random'\,' quadratic equation: \[
A x^2 + 2B x + C = 0.
\] What is the probability that the above equation has real root?

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

The event that the random quadratic equation has real roots is
equivalent to the event that \(4B^2 - 4AC \geq 0\) i.e.~\(B^2 \geq AC.\)
Therefore the probability that the above equation has real roots is: \[
\begin{eqnarray*}
    P(B^2 \geq AC) &=& P(2\log B \geq \log A + \log C) \\
    &=& P(-2Y_2 \geq -Y_1 - Y_3) \quad \text{where\quad$Y_1 = -  \log A, Y_2 = -  \log B, Y_3 =  - \log C$ are iid Expo's.}\\
    &=& P( 3 Y_2 \leq Y_1 + Y_2 + Y_3    )\\
    &=& P\left(  \frac{Y_2}{Y_1 + Y_2 + Y_3}  \leq \frac{1}{3}   \right) \\
    &=& P\left( \text{Beta}(1,2) \leq \frac{1}{3} \right)  \quad \text{since}\quad\frac{Y_2}{Y_1 + Y_2 + Y_3} \sim \text{Beta}(1,2)  \\
    &=& P\left(  \text{Beta}(2,1) \geq \frac{2}{3}   \right) \quad \text{since}\quad\text{Beta}(1,2) \sim 1 -  \text{Beta}(2,1) \\
    &=& P\left( U^{1/2} \geq \frac{2}{3}  \right)  \quad \text{since}\quad   \text{Beta}(2,1) \sim U^{1/2} \\
    &=& P\left( U \geq \frac{4}{9}  \right)  \\
    &=& \frac{5}{9}.
\end{eqnarray*}
\] Thus, \(2S\sqrt{1+C^2} \sim 2S/\sqrt{B}\) follows.

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-1}{%
\subsection*{✏️ Section Problem 2}\label{section-problem-2-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2}

Assume that \(U_1, \cdots, U_n \overset{i.i.d}{\sim}\) Unif,
\(Z_1, \cdots, Z_{2n} \overset{i.i.d}{\sim} \mathcal{N}(0,1)\), and the
\(U\)'s and \(Z\)'s are independent. Define \[
X = \frac{Z_1^2 + \cdots + Z_m^2}{ Z_1^2 + \cdots + Z_{2n}^2},
\] where \(m< 2n\). Find the distribution of
\(Y = (U_1 U_2 \cdots U_n)^{-X}\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let us first find the distribution of \(\log Y\). Now, \$ \log Y = - X
\log (U\_1 U\_2 \cdots U\_n) = X \sum\_\{i=1\}\^{}n ( - \log U\_i)\$.
Denoting \(G^* = \sum_{i=1}^n ( - \log U_i)\), we have
\(\log Y = XG^*\). \textbackslash{} \underline{Step-1:} We note that
\(G^*\sim \text{Gamma} (n)\) and \(X \perp\!\!\!\!\perp G^*\) (since
\(U\)'s and \(Z\)'s are independent).\textbackslash{}

Now, \(Z_i^2\sim \chi^2_1 \sim 2 \text{Gamma}(\frac{1}{2})\) for all
\(i \in \{1,2,...,2n\}\) and \(Z_i\)'s are independent. Thus, we can
represent \(X\) as \[
X\sim \frac{G_{\frac{m}{2}}}{ G_{\frac{m}{2}} + G_{n-\frac{m}{2}}} 
\] where \$ G\_\{\frac{m}{2}\} \sim \text{Gamma}(\frac{m}{2})\$,
\(G_{n-\frac{m}{2}}\sim \text{Gamma}(n-\frac{m}{2})\) and
\(G_{\frac{m}{2}} \perp\!\!\!\!\perp G_{n-\frac{m}{2}}\) . This gives us
the following.\textbackslash{} \underline{Step-2:}
\(X \sim \text{Beta}(\frac{m}{2},n-\frac{m}{2})\)

Now, we know that
\(G_{\frac{m}{2}} + G_{n-\frac{m}{2}} \sim \text{Gamma}(n)\) and
\((G_{\frac{m}{2}} + G_{n-\frac{m}{2}}) \perp\!\!\!\!\perp \frac{G_{\frac{m}{2}}}{ G_{\frac{m}{2}} + G_{n-\frac{m}{2}}}\).
This observation along with Step-1 and Step-2 implies
\[(X,G^*) \sim  \Big(\frac{G_{\frac{m}{2}}}{ G_{\frac{m}{2}} + G_{n-\frac{m}{2}}},G_{\frac{m}{2}} + G_{n-\frac{m}{2}} \Big)\]
\%Since \(G_n \sim \text{Gamma}(m/2) + \text{Gamma}(n-m/2)\) and the
representation preserves the independence between \(X\) and \(G_n\), we
have Taking the product of the components of the random vectors on both
sides, we get\textbackslash{} \underline{Step-3:}
\(\log Y \sim G_{m/2}\)\textbackslash{}

Step-3 implies, \(Y \sim e^{G_{m/2}}\). Therefore, the distribution of
\(Y\) is same as the distribution of the exponential of a
Gamma\((\frac{m}{2})\) random variable.

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-1}{%
\subsection*{✏️ Section Problem 3}\label{section-problem-3-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3}

Let \(Z_1, Z_2 \overset{i.i.d}{\sim} \mathcal{N}(0,1)\). Furthermore,
let \(X_1,X_2 \overset{i.i.d}{\sim} \chi_1^2\). Show that \[
Z_1Z_2\sim \frac{1}{2} (X_1 - X_2).
\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Firstly, \((X_1,X_2) \sim (Z^2_1,Z^2_2)\). Therefore, we have \[
\frac{1}{2} (X_1 - X_2) \sim \frac{1}{2}(Z_1^2 - Z_2^2) = \frac{1}{2} (Z_1 + Z_2) (Z_1 - Z_2) .
\] Since \(Z_1 + Z_2\sim N(0, 2)\), \(Z_1 - Z_2\sim N(0, 2)\) and
\((Z_1 + Z_2)\perp\!\!\!\!\perp (Z_1 - Z_2)\) (this is one of the many
special properties of the Normal distribution), we can jointly represent
them as \((Z_1 + Z_2, Z_1-Z_2) \sim (\sqrt{2} Z_1, \sqrt{2} Z_2)\).
Consequently, we have \[
\frac{1}{2} (X_1 - X_2) \sim \frac{1}{2} \sqrt{2} Z_1 \sqrt{2} Z_2  = Z_1 Z_2.
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-4-1}{%
\subsection*{✏️ Section Problem 4}\label{section-problem-4-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 4}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Let \(X_1, X_2 \overset{i.i.d}{\sim} Expo\). Show that
  \(X_1 - X_2 \sim \operatorname{Laplace}\).
\item
  Let \(Z_1,Z_2,Z_3,Z_4 \overset{i.i.d}{\sim} \mathcal{N}(0,1)\), and
  \(L \sim\) Laplace. Show that \[
  Z_1 Z_2 - Z_3 Z_4 \sim L.
  \]
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Define \(X_1,X_2,X_3,X_4\) as \textit{i.i.d} \(\chi^2_1\) random
variables. This gives us the representation
\((X_1,X_2,X_3,X_4) \sim (Z^2_1, Z^2_2, Z^2_3, Z^2_4)\) . Now, using
problem 2.1 of this section, we have \[
Z_1 Z_2 - Z_3 Z_4 \sim \frac{1}{2} \left\{   (X_1 - X_2)  - (X_3 - X_4)    \right\} 
\sim \frac{1}{2} (Z_1^2 + Z_3^2) - \frac{1}{2} (Z_2^2 + Z_4^2).
\] By additivity of the \(\chi^2\) distribution, we have
\[\frac{Z_1^2 + Z_3^2}{2} \sim \frac{\chi_2^2}{2}  \sim \text{Gamma}(1) \sim \text{Expo}\sim Y_1 
;\hspace{0.2cm} \frac{Z_2^2 + Z_4^2}{2} \sim \frac{\chi_2^2}{2}\sim \text{Gamma}(1) \sim \text{Expo} \sim Y_2\]
, where \(Y_1\) and \(Y_2\) are independent Expo random variables. Part
\((i)\) of Exercise \(3.7\) implies \[
Z_1 Z_2 - Z_3 Z_4 \sim  Y_1 - Y_2 \sim L.
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-5-1}{%
\subsection*{✏️ Section Problem 5}\label{section-problem-5-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 5}

Let \(Y\) have a Cauchy distribution centered at \(\theta\), i.e.~the
density of \(Y\) is
\[f(y|\theta) = \frac{1}{\pi} \frac{1}{1+(y-\theta)^2} ,\hspace{0.2cm} y \in \mathbb{R} \]
Suppose that \(\theta\) has a Cauchy distribution (centered at \(0\)).
Find the marginal distribution of \(Y\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(\mathbb{R}_+ = (0,\infty)\) be the positive half of the real line.
\[
\begin{eqnarray*}
P(W>a+b|W>a) &=& P(X^\beta>a+b|X^\beta>a) \quad\text{where $X\sim \text{Expo}$}\nonumber\\
&=& P(X>(a+b)^{1/\beta}|X>a^{1/\beta}) \quad\text{since $x\mapsto x^{1/\beta}$ is strictly increasing on $\mathbb{R}_+$ for $\beta>0$}\nonumber\\
&=& P(X>(a+b)^{1/\beta}-a^{1/\beta})\quad\text{by memoryless property of Expo}\nonumber\\
&>& P(X>b^{1/\beta}) \quad\text{since $1/\beta\in (0,1)$ and $(a+b)^r < a^r + b^r$, $\forall a>0$, $b>0$, $r\in (0,1)$} \nonumber\\
&=& P(W > b) \quad\text{since $x\mapsto x^{\beta}$ is increasing on $\mathbb{R}_+$ for $\beta>0$}\nonumber
\end{eqnarray*}
\]

\emph{Note:} Proof of \((a+b)^r < a^r + b^r\) for all \(a>0\),\(b>0\),
\(r\in (0,1)\).\textbackslash{}

We have: \[
\begin{eqnarray}
(a+b)^r = (a+b)(a+b)^{r-1} = a(a+b)^{r-1} + b(a+b)^{r-1}\nonumber
\end{eqnarray}
\] By assumption, \(r-1<0\) so that \(x\mapsto x^{r-1}\) is decreasing
on \(\mathbb{R}_+\). Therefore, \((a+b)^{r-1} < a^{r-1}\) and
\((a+b)^{r-1} < b^{r-1}\).

Thus, \[
\begin{eqnarray}
(a+b)^r < a^r + b^r\nonumber
\end{eqnarray}
\] for all \(a>0\),\(b>0\), \(r\in (0,1)\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-6-1}{%
\subsection*{✏️ Section Problem 6}\label{section-problem-6-1}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 6}

The people of Lineland live on the unit interval \([0,1]\). They love
coffee. Currently, they have 2 Starbucks stores, at the points 0 and 1.
Starbucks decides to open new stores in Lineland, according to a Poisson
process of rate \(\lambda\) on the interval \([0,1]\). Let \(N\) be the
number of new Starbucks stores in Lineland (i.e., not including the
existing stores at the points 0 and 1 ). Let \(W\) be the furthest
distance that a Lineland citizen could ever have to walk to get to the
nearest Starbucks, after the new stores have opened. Find
\(E(W \mid N=n)\), where \(n\) is a positive integer. Simplify fully,
expressing your answer in terms of a harmonic number \(H_m\) for some
\(m\), where \[
H_m=\sum_{k=1}^m \frac{1}{k} .
\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Given \(N=n\), the locations of the \(n\) new stores are i.i.d. Unif.
The furthest distance someone could have to walk is half of the largest
spacing between two stores. By the same method in class used on the
cutting a rope problem (using the joint representations for the order
statistics of Uniforms and of Exponentials), the expected largest
spacing between two stores is \(\frac{1}{n+1} H_{n+1}\). Thus, \[
E(W \mid N=n)=\frac{1}{2(n+1)} H_{n+1} .
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-7}{%
\subsection*{✏️ Section Problem 7}\label{section-problem-7}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 7}

Consider the following simple model for the growth of a population of
bacteria. Any individual bacterium splits into two bacteria at some
random time, independently. It takes an Exponential amount of time for
any specific bacterium to split (measured from the time of birth of that
bacterium, and choosing the units in which time is measured so that the
Expo has mean 1). So each individual bacterium has its own Expo waiting
time until it splits, and these Expo r.v.s are i.i.d.

At time 0 , there is one bacterium. Let \(T_n\) be the time (on a
timeline) of the \(n\)th splitting occurrence. So \(T_1<T_2<\ldots\),
with \(T_1\) the time at which the bacterium that was present at time 0
splits, \(T_2\) the time of the next splitting occurrence, etc.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Find the CDF of \(T_n\).
\item
  Find the distribution of the number of bacteria present at time \(t\),
  for any \(t>0\).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Note that the first interarrival time is Expo, the second interarrival
  is \(\frac{1}{2}\) Expo, and in general the \(j\) th interarrival time
  is \(\frac{1}{j}\) Expo. By the Rényi representation, \(T_n\) is
  distributed as the maximum of \(n\) i.i.d. Expos. Therefore, \[
  P\left(T_n \leq t\right)=\left(1-e^{-t}\right)^n .
  \]
\item
  Let \(S_t\) be the number of splittings up until time \(t\). By the
  count-time duality, \[
  P\left(S_t=n\right)=P\left(S_t<n+1\right)-P\left(S_t<n\right)=P\left(T_{n+1}>t\right)-P\left(T_n>t\right) .
  \] By the result of (a), \[
  P\left(S_t=n\right)=\left(1-e^{-t}\right)^n-\left(1-e^{-t}\right)^{n+1}=e^{-t}\left(1-e^{-t}\right)^n .
  \] Interestingly, this shows that \[
  S_t \sim \operatorname{Geom}\left(e^{-t}\right) .
  \] The number of bacteria at time \(t\) is \[
  S_t+1 \sim \mathrm{FS}\left(e^{-t}\right)
  \]
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-8}{%
\subsection*{✏️ Section Problem 8}\label{section-problem-8}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 8}

In a certain town, each married couple has a Poisson \((\lambda)\)
number of children, with \(\lambda\) unknown. An anthropologist picks a
sample of couples and observes \(Y_1, \ldots, Y_n\), where \(Y_j\) is
the number of children of the \(j\) th couple and it is assumed that
\(Y_j \sim \operatorname{Pois}(\lambda)\) independently. The
anthropologist wishes to estimate the probability of a couple being
childless, i.e., \(p_0 \equiv P\left(Y_j=0\right)\). Let \(\bar{Y}\) be
the sample mean of \(Y_1, \ldots, Y_n\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Find \(E\left(Y_1 \mid \bar{Y}\right)\) and the conditional
  distribution of \(Y_1\) given \(\bar{Y}\).
\item
  The anthropologist proposes estimating \(p_0\) using the proportion of
  couples that are childless, i.e., the number of childless couples
  divided by \(n\). Call this estimator \(T\). It will be shown later in
  Stat \(210 / 211\) that a better estimator can be obtained by
  conditioning on \(\bar{Y}\). Find a simple expression for
  \(E(T \mid \bar{Y})\) (this new estimator should be computable without
  knowing \(\lambda\) ).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Using the result for the conditional distribution of a Poisson given
  the sum of itself and another Poisson, \[
  Y_1\left|\bar{Y} \sim Y_1\right|\left(n \bar{Y}=Y_1+\cdots+Y_n\right) \sim \operatorname{Bin}(n \bar{Y}, 1 / n) .
  \] So \(E\left(Y_1 \mid \bar{Y}\right)=\bar{Y}\), which is true in
  general (not just for the Poisson), by symmetry and linearity as in
  Example 9.3.6 in the Stat 110 book.
\item
  Let \(T=\left(I_1+\cdots+I_n\right) / n\), where \(I_j\) is the
  indicator for the \(j\) th couple being childless. By linearity and
  symmetry, \[
  \begin{align*}
  \mathrm{E}(T \mid \bar{Y})
  &=\frac{1}{n} \mathrm{E}\left(I_1+\cdots+I_n \mid \bar{Y}\right) \\ 
  &=\mathrm{E}\left(I_1 \mid \bar{Y}\right)=P\left(Y_1=0 \mid \bar{Y}\right)
  \\
  &=\left(1-\frac{1}{n}\right)^{n \bar{Y}}
  \end{align*}
  \]
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-1}{%
\section*{Next Week}\label{next-week-1}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  On Order Statistics
\item
  Meaning on Mean
\end{itemize}

\includegraphics{./assets/img/renyi.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-3}{%
\chapter*{Section 3}\label{section-3}}
\addcontentsline{toc}{chapter}{Section 3}

\markboth{Section 3}{Section 3}

Last Updated: 30 Sept 2023

Date: 29 Sept 2023

\hypertarget{introduction-2}{%
\section*{Introduction}\label{introduction-2}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  Order Statistics
\item
  Renyi Representation
\end{itemize}

\hypertarget{order-statistics}{%
\section*{Order Statistics}\label{order-statistics}}
\addcontentsline{toc}{section}{Order Statistics}

\markright{Order Statistics}

\leavevmode\vadjust pre{\hypertarget{def-poisson-process}{}}%
\begin{definition}[]\label{def-poisson-process}

The \emph{order statistics} of r.v.s \(Y_1, \cdots, Y_n\) are the sorted
list of the \(Y_j\), denoted by
\(Y_{(1)}, \leq Y{(2)} \leq \cdots Y{(n)}\). For example,
\[Y{(1)} =\min(Y_1,\cdots, Y_n), Y{(n)}=\max(Y_1,\cdots, Y_n)\] and for
\(n\) odd \(Y_{((n+1)/2)}\) is the median of \(Y_1,\cdots, Y_n\).

\end{definition}

\leavevmode\vadjust pre{\hypertarget{thm-competing-risk}{}}%
\begin{theorem}[]\label{thm-competing-risk}

Let \(Y_1 = X_1/\lambda_1\) and \(Y_2 = X_2/\lambda_2\) be independent
(scaled) Exponentials, with \(X_1,X_2 \sim\)Expo and
\(\lambda_1, \lambda_2 >0\) constants. Define
\[W\equiv\min(Y_1, Y_2) \text{ and } B_0\equiv I_{Y_1<Y_2}\] where
\(I_A\) is the indicator random variable for an event \(A\). Then
\(W \perp\!\!\!\perp B_0\).

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-lemma}{}}%
\begin{theorem}[]\label{thm-lemma}

Let \(0<p<1\) be a constant and \(U\sim\)Unif. Define
\[B\equiv I_{U\leq p} \text{ and } M\equiv \min(\frac{U}{p}, \frac{1-U}{1-p})\].
Then, the indicator r.v \(B\sim\) Bern\((p)\) is independent of
\(M \sim \text{ Unif}\).

\end{theorem}

\hypertarget{renyi-representation}{%
\section*{Renyi Representation}\label{renyi-representation}}
\addcontentsline{toc}{section}{Renyi Representation}

\markright{Renyi Representation}

\leavevmode\vadjust pre{\hypertarget{thm-renyi-representation}{}}%
\begin{theorem}[]\label{thm-renyi-representation}

For \(Y_1,\cdots Y_n\) i.i.d Expo, the \emph{order statistics}
\((Y_{(1)},\cdots, Y_{(n)})\) can be jointly represented as
\[Y_{(k)}\sim \sum_{j=1}^k \frac{1}{n-j+1} X_j\], where the \(X_j\)'s
are also i.i.d Exponentials.

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-uniform-order-statistics}{}}%
\begin{theorem}[]\label{thm-uniform-order-statistics}

Let \(U_1, \cdots U_n\) be i.i.d Uniform and
\[W_j =\frac{X_1+\cdots+X_j}{X_1+\cdots+X_{n+1}}\] with
\(X_1,\cdots X_{n+1}\) i.i.d Expo. Then we have the following joint
representation for the order statistics of the \(U_j\):
\[(U_{(1)}, \cdots U_{(n)}) \sim (W_1, \cdots, W_n).\]

\end{theorem}

\hypertarget{section-discussion-questions-1}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions-1}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

\hypertarget{section-problem-1-2}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

There is a restaurant with only one seat, and between 1pm to 3pm
inclusive, exactly two people want to eat there. They each arrive at the
restaurant on a uniform time between 1pm and 3pm inclusive. If the seat
is empty, they take the seat and eat for the next 20 minutes. Otherwise,
they leave out out of frustration. What is the probability that both
people get to enjoy their 20 minute meal?

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Scale the problem so that the two restaurateurs arrive on {[}0, 1{]}.
Then, the problem basically asks for P(U\_(1) + 1/6 \textless{} U\_(2)),
where U\_1 and U\_2 are iid Unif. By the resny representation, if we
have X\_1,\ldots,X\_3 iid Expo, we want P(X\_2 / (X\_1 + X\_2 + X\_3
\textgreater{} 1/6), i.e.~P(B \textgreater{} 1/6) for B following
Beta(1,2). We know that B = 1 - U\^{}\{1/2\} for U Unif, so substituting
this in and doing easy manipulations yields the answer 25/36.

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-2}{%
\subsection*{✏️ Section Problem 2}\label{section-problem-2-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2}

Let \(U_{1}, \cdots, U_{n-1}\) be i.i.d Unif (where \(n \geq 4\)). Use
these points to break the interval \([0,1]\) into \(n\) subintervals.
\[[0, U_{(1)}], [U_{(1)}, U_{(2)}], \cdots, [U_{(n-1)}, U_{(n-1)}], [U_{(n-1)}, 1]\]

Let \(L_j\) be the length of the \(j\)th subinterval from the above
list, and \(R=L_3/L_2\)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Find a function \(g\) (which takes on a very simple from rather than
  looking messy), such that \(R\sim g(U_1)\)
\item
  Find \(E(R)\) if it is finite; otherwise, show that it is infinite.
\item
  Find \(E(\sqrt{R})\) if it is finite; otherwise, show that it is
  infinite.
\item
  Find the distribution of the difference in length between the longest
  of these subintervals and the second longest of these subintervals.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(X_1, \ldots, X_n\) be i.i.d. Expo, and \(T=X_1+\cdots+X_n\). By
the representation for the Uniform order statistics, \[
U_{(j)} \sim \frac{X_1+\cdots+X_j}{T},
\] where this is a representation for the joint distribution of
\(\left(U_{(1)}, \ldots, U_{(n-1)}\right)\). Then \[
R=\frac{U_{(3)}-U_{(2)}}{U_{(2)}-U_{(1)}} \sim \frac{X_3}{X_2} .
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  We have \[
  R \sim \frac{X_3}{X_2}=\frac{X_3 /\left(X_2+X_3\right)}{X_2 /\left(X_2+X_3\right)} \sim \frac{U}{1-U}
  \] where \(U \sim\) Unif.
\item
  The expectation of \(R\) is infinite since \[
  E\left(\frac{X_3}{X_2}\right)=E\left(X_3\right) E\left(\frac{1}{X_2}\right)=E\left(\frac{1}{X_2}\right)=\int_0^{\infty} x^{-1} e^{-x} d x=\infty .
  \] The integral diverges to \(\infty\) (which is why \(\Gamma(0)\) is
  undefined) since \(\int_0^1 x^{-1} e^{-x} d x\) diverges to
  \(\infty\), because \(\int_0^1 x^{-1} d x\) diverges to \(\infty\) and
  \(x^{-1} e^{-x} \geq x^{-1} e^{-1}\) for \(0<x<1\).
\item
  Using the result from the book for Gamma moments, \[
  E(\sqrt{R})=E\left(X_3^{1 / 2}\right) E\left(X_2^{-1 / 2}\right)=\Gamma(1+0.5) \Gamma(1-0.5)=\frac{\pi}{2},
  \] since \[
  \Gamma(0.5)=\sqrt{\pi}, \Gamma(1.5)=0.5 \cdot \Gamma(0.5) .
  \]
\item
  The joint distribution of \(L_1, L_2, \ldots, L_n\) is the same as the
  joint distribution of \[
  \frac{X_1}{T}, \frac{X_2}{T}, \ldots, \frac{X_n}{T} .
  \] Conveniently, these all have the same denominator \(T\). We need
  the distribution of \[
  \frac{X_{(n)}-X_{(n-1)}}{T}=\frac{X_{(n)}-X_{(n-1)}}{X_{(1)}+\cdots+X_{(n)}} .
  \] By the Rényi representation, the desired distribution is \[
  \begin{align*}
  \frac{\left(\frac{1}{n} Y_1+\frac{1}{n-1} Y_2+\cdots+Y_n\right)-\left(\frac{1}{n} Y_1+\frac{1}{n-1} Y_2+\cdots+\frac{1}{2} Y_{n-1}\right)}{Y_1+Y_2+\cdots+Y_n}  &=\frac{Y_n}{Y_1+Y_2+\cdots+Y_n} \\
  &\sim \operatorname{Beta}(1, n-1),
  \end{align*}
  \] where the \(Y_j\) are i.i.d. Expo. (For the denominator, note that
  the term \(\frac{1}{n-j+1} Y_j\) appears in exactly \(n-j+1\) of the
  representations for the individual order statistics, resulting in
  \(Y_j\) when all the order statistics get added up.)
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-2}{%
\subsection*{✏️ Section Problem 3}\label{section-problem-3-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  (Fun with Order Statistics) Let \(U_1, \ldots, U_n\) be i.i.d. Unif,
  and let \(U_{(1)} \leq U_{(2)} \leq\) \(\cdots \leq U_{(n)}\) be their
  order statistics. In the parts below, be sure to fully justify any
  claims of independence.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  For \(1 \leq j<k<l<m \leq n\), find the joint distribution of \[
  \frac{U_{(j)}}{U_{(k)}}, \frac{U_{(k)}}{U_{(l)}}, \frac{U_{(l)}}{U_{(m)}} .
  \]
\item
  Find \(\operatorname{Cov}\left(\log U_{(i)}, \log U_{(j)}\right)\) for
  \(1 \leq i<j \leq n\) (as a sum of simple terms).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  As given in class and on the homework, we can jointly represent \[
  \left(U_{(1)}, U_{(2)}, \ldots, U_{(n)}\right) \sim\left(\frac{S_1}{S_{n+1}}, \ldots, \frac{S_n}{S_{n+1}}\right),
  \] where \(S_j \equiv X_1+\cdots+X_j\) and \(X_1, \ldots, X_{n+1}\)
  are i.i.d. Expo. Then \[
  \left(\frac{U_{(j)}}{U_{(k)}}, \frac{U_{(k)}}{U_{(l)}}, \frac{U_{(l)}}{U_{(m)}}\right) \sim\left(\frac{S_j}{S_k}, \frac{S_k}{S_l}, \frac{S_l}{S_m}\right) .
  \] So
  \(\frac{S_j}{S_k} \sim \operatorname{Beta}(j, k-j), \frac{S_k}{S_l} \sim \operatorname{Beta}(k, l-k), \frac{S_l}{S_m} \sim \operatorname{Beta}(l, m-l)\)
  by Gamma-Beta representations. By the Gamma-Beta independence result
  in Proposition 3.7.8, \(\frac{S_j}{S_k}, \frac{S_k}{S_l}, S_l\) are
  independent. Write \(S_m=S_l+T\) with \(T \equiv X_{l+1}+\cdots+X_m\)
  independent of \(\left(S_j, S_k, S_l\right)\). Then
  \(\left(T, S_l\right)\) is independent of
  \(\left(\frac{S_j}{S_k}, \frac{S_k}{S_l}\right)\), so
  \(\frac{S_j}{S_k}, \frac{S_k}{S_l}, \frac{S_l}{S_m}\) are independent.
\item
  We have \[
  \operatorname{Cov}\left(\log U_{(i)}, \log U_{(j)}\right)=\operatorname{Cov}\left(-\log U_{(i)},-\log U_{(j)}\right)=\operatorname{Cov}\left(X_{(n-i+1)}, X_{(n-j+1)}\right),
  \] where \(X_1, \ldots, X_n\) are i.i.d. Expo (the order of the order
  statistics is reversed since \(-\log\) is decreasing). The Rényi
  representation yields the joint representation \[
  \left(X_{(n-i+1)}, X_{(n-j+1)}\right) \sim\left(\sum_{k=1}^{n-i+1} \frac{1}{n-k+1} X_k, \sum_{k=1}^{n-j+1} \frac{1}{n-k+1} X_k\right) .
  \] So \[
  \operatorname{Cov}\left(X_{(n-i+1)}, X_{(n-j+1)}\right)=\sum_{k=1}^{n-j+1} \frac{1}{(n-k+1)^2}=\sum_{k=j}^n \frac{1}{k^2} .
  \] Interestingly, this depends on \(j\) but the \(i\) disappeared!
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-4-2}{%
\subsection*{✏️ Section Problem 4}\label{section-problem-4-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 4}

Let \(X_1, X_2, \ldots, X_n\) be i.i.d. Expo. As usual, let
\(X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}\) be their order
statistics. Let \[
T=\sum_{j=1}^n\left(X_j-X_{(1)}\right) .
\] Find the distribution of \(T\). Simplify fully; you can give the name
and parameters (if it is a distribution we have studied), or the PDF.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By the Rényi representation, we can represent
\(\left(X_{(1)}, \ldots, X_{(n)}\right)\) jointly as \[
X_{(k)} \sim \sum_{j=1}^k \frac{1}{n-j+1} Y_j
\] where the \(Y_j\) 's are also i.i.d. Expo. Then \[
T=\sum_{k=1}^n X_k-n X_{(1)}=\sum_{k=1}^n X_{(k)}-n X_{(1)} \sim \sum_{k=1}^n Y_j-Y_1=\sum_{k=2}^n Y_j \sim \operatorname{Gamma}(n-1) \text {. }
\] Method 2: Consider \(n\) independent Poisson processes, each with
rate 1 . Let \(X_j\) be the time of the first arrival in the \(j\) th of
these processes. Given that \(X_{(1)}=a\), one of the differences
\(X_1-a, X_2-a, \ldots, X_n-a\) is 0 , and by the memoryless property
the rest are i.i.d. Expo r.v.s. Thus, \[
T\left|\left(X_{(1)}=a\right) \sim \sum_{k=1}^n\left(X_k-a\right)\right|\left(X_{(1)}=a\right) \sim \operatorname{Gamma}(n-1) .
\] This conditional distribution doesn't depend on \(a\), so we also
have \(T \sim \operatorname{Gamma}(n-1)\) unconditionally

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-5-2}{%
\subsection*{✏️ Section Problem 5}\label{section-problem-5-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 5}

Let \(U_1, \ldots, U_n\) be i.i.d. Unif, and let
\(U_{(1)} \leq U_{(2)} \leq \cdots \leq U_{(n)}\) be their order
statistics. Show without using calculus that for
\(1 \leq i \leq j \leq n\), \[
\operatorname{Cov}\left(U_{(i)}, U_{(j)}\right)=\frac{i(n-j+1)}{(n+1)^2(n+2)} .
\] Hint: Use representation and properties of covariance. Note that for
r.v.s of the form \[
W_k=\frac{X_k}{X_1+\cdots+X_{n+1}},
\] the sum \(W_1+\cdots+W_{n+1}\) has variance 0 .

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(X_1, X_2, \ldots, X_{n+1}\) be i.i.d. Expo and \(W_k\) be as in
the hint. Recall the joint representation \[
\left(U_{(1)}, U_{(2)}, \ldots, U_{(n)}\right) \sim\left(W_1, W_1+W_2, \ldots, W_1+W_2+\cdots+W_n\right) .
\] By bilinearity, \[
\operatorname{Cov}\left(U_{(i)}, U_{(j)}\right)=\operatorname{Cov}\left(W_1+\cdots+W_i, W_1+\cdots+W_j\right)=i \operatorname{Var}\left(W_1\right)+i(j-1) \operatorname{Cov}\left(W_1, W_2\right) .
\] By the Beta-Gamma algebra, \[
W_1 \sim \operatorname{Beta}(1, n)
\] so \[
E\left(W_1\right)=\frac{1}{n+1}, \operatorname{Var}\left(W_1\right)=\frac{n}{(n+1)^2(n+2)} .
\] To find \(\operatorname{Cov}\left(W_1, W_2\right)\), we will use the
hint. Note that \(W_1+\cdots+W_{n+1}\) is the constant 1 , so it has
variance 0 . Therefore, \[
0=\operatorname{Var}\left(W_1+\cdots+W_{n+1}\right)=(n+1) \operatorname{Var}\left(W_1\right)+(n+1) n \operatorname{Cov}\left(W_1, W_2\right),
\] which gives \[
\operatorname{Cov}\left(W_1, W_2\right)=-\frac{1}{n} \operatorname{Var}\left(W_1\right)
\] Hence, \[
\operatorname{Cov}\left(U_{(i)}, U_{(j)}\right)=\left(i-\frac{i(j-1)}{n}\right) \operatorname{Var}\left(W_1\right)=\frac{i(n-j+1)}{(n+1)^2(n+2)}
\].

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-6-2}{%
\subsection*{✏️ Section Problem 6}\label{section-problem-6-2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 6}

Assume that \(U_1, \cdots, U_n \sim_{iid} \textnormal{Unif}\), and
\(U_{(1)},U_{(2)},\cdots, U_{(n)}\) are order statistics.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  For \(1 \leq j < k < l < m \leq n\), find the joint distribution of
  \(\left( \frac{U_{(j)}}{U_{(k)}}, \frac{U_{(k)}}{U_{(l)}}, \frac{U_{(l)}}{U_{(m)}}\right).\)
\item
  (Reconstructing Uniforms) Show that
  \[\left( \frac{U_{(1)}}{U_{(2)}}, \left(\frac{U_{(2)}}{U_{(3)}}\right)^2,\cdots. \left(\frac{U_{(n-1)}}{U_{(n)}}\right)^{n-1}, U_{(n)}^n \right)\]
  are i.i.d. Uniform random variables.
\item
  Find \(\mathbb{E}\left(\prod_{i=1}^{n} U^{\alpha_i}_{(i)}\right)\),
  where \(\alpha_i\)'s are positive.
\item
  Find \(\mathrm{Cov}\left(\log U_{(i)}, \log U_{(j)}\right)\) for
  \(j > i\).
\item
  What is the conditional distribution of \(U_{(i)} | U_{(j)}\) for
  \(j > i\)?
\item
  Does the conditional distribution \(U_{(i)}\left|U_{(j)}\right.\)
  change if we condition on more order statistics \(U_{(k)}\) with
  \(k > j\)? For example, find the conditional distribution of
  \(U_{(i)}| (U_{(i+1)},\cdots, U_{(n)}).\)
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Let \(X_1,\ldots,X_{n+1}\) be i.i.d. Exponential r.v.s. Using the
  representation of Uniform order statistics, we can jointly represent
  \(\left( \frac{U_{(j)}}{U_{(k)}}, \frac{U_{(k)}}{U_{(l)}}, \frac{U_{(l)}}{U_{(m)}}\right)\)
  with \$ \left( \frac{S_j}{S_k}, \frac{S_k}{S_l},\frac{S_l}{S_m}
  \right) \$ where \(S_j = \sum_{i=1}^j X_i\) are the partial sums.
  Proposition 3.7.9 says that
  \(\left( \frac{S_j}{S_k}, \frac{S_k}{S_l},S_l \right)\) fully
  independent. Because \(X_i\)'s are i.i.d., we must have
  \[\left( \frac{S_j}{S_k}, \frac{S_k}{S_l},S_l \right) \perp\!\!\!\perp X_{l+1} + \cdots X_m = S_m - S_l .\]
  This implies
  \[ \left( \frac{S_j}{S_k}, \frac{S_k}{S_l} \right) \perp\!\!\!\perp \left( S_l, S_m - S_l\right).\]
  Since \(\frac{S_l}{S_m}\) is a function of
  \(\left( S_l, S_m - S_l\right)\), we have \$ \left( \frac{S_j}{S_k},
  \frac{S_k}{S_l} \right) \perp!!!\perp \frac{S_l}{S_m}\$ and since the
  left-hand side consists of independent random variables we must have
  \(\left( \frac{S_j}{S_k}, \frac{S_k}{S_l},\frac{S_l}{S_m}\right)\)
  fully independent by Problem 2 from HW1.
\item
  First we show that the r.v.s. are Uniform. Let
  \(X_i \sim_{iid} \textnormal{Expo}.\) For each \(1 \leq i \leq n-1\)
  we have
  \[ \left(\frac{U_{(i)}}{U_{(i+1)}}\right)^i  = \left(\frac{X_1 + X_2 + \cdots + X_i}{X_1+\cdots+X_{i+1}} \right)^i  \sim \textnormal{Beta}(i,1)^i
  ,\] through representation of Uniform order statistics and of Beta
  distribution. Because
  \(\textnormal{Beta}(\alpha,1) \sim \textnormal{Unif}^{1/\alpha}\) for
  all \(\alpha >1\) we have
  \(\left(\frac{U_{(i)}}{U_{(i+1)}}\right)^i \sim \textnormal{Unif}.\)
  To establish independence, we will use induction. It suffice to show
  that if \(X_i \overset{iid}{\sim} \textnormal{Expo}\) and
  \(S_j = \sum_{i=1}^j X_i\) denote the partial sums, then \[
  \begin{equation}
  \left(\frac{S_1}{S_{2}}, \frac{S_2}{S_3},\cdots,\frac{S_n}{S_{n+1}},  S_{n+1}\right)
  \end{equation}
  \] is fully independent. Note that our induction is the exponential
  r.v.s. instead of on the order statistics. The base case \(n = 3\) is
  a direct consequence of part (1). For the induction step assume
  equation above is true for \(n = k\). Using independence of the Expo
  r.v.s.,we have
  \[ \left(\frac{S_1}{S_{2}}, \frac{S_2}{S_3},\cdots,\frac{S_k}{S_{k+1}}, S_{k+1}\right) \perp\!\!\!\perp X_{k+2}.\]
  Using similar arguments in part (1) we will have \$
  \left(\frac{S_1}{S_{2}}, \frac{S_2}{S_3},\cdots,\frac{S_k}{S_{k+1}}
  \right) \perp!!!\perp \left(S\_\{k+1\}, X\_\{k+2\}\right)\$ and
  therefore
  \[ \left(\frac{S_1}{S_{2}}, \frac{S_2}{S_3},\cdots,\frac{S_k}{S_{k+1}} \right) \perp\!\!\!\perp \left( \frac{S_{k+1}}{S_{k+2}}, S_{k+2} \right).\]
  Notice that the left and right hand sides are both fully independent
  by induction hypothesis and Beta-Gamma respectively. By definition of
  independence of r.v.s., we must have Equation above valid for
  \(n = k+1.\)
\item
  Let \(W_i = U_{(i)}^i \overset{iid}{\sim} \textnormal{Unif}\) by part
  2). Then
  \(\left(W_i\right)^{\beta} \sim \textnormal{Unif}^\beta \sim \textnormal{Beta}(1/\beta, 1)\)
  for any \(\beta >0\), and the expectation is
  \(\mathbb{E}[W_i^{\beta}] = \frac{1/\beta}{1/\beta +1} = \frac{1}{1+\beta}.\)
  We can re-express the product as product of independent r.v.s using
  Part 2) because \[
  \begin{align*}
  \prod_{i=1}^n U_{(i)}^{\alpha_i}  &= U_{(1)}^{\alpha_1} \cdot U_{(2)}^{\alpha_2}\cdots U_{(n)}^{\alpha_n} \\
  &= \left(\frac{U_{(1)}}{U_{(2)}}\right)^{\alpha_1}\left(\frac{U_{(2)}}{U_{(3)}}\right)^{\alpha_1 + \alpha_2} \cdots U_{(n)}^{\alpha_1+\cdots + \alpha_n}\\
  &= W_1^{\alpha_1} W_2^{(\alpha_1+\alpha_2)/2} \cdots W_n^{(\alpha_1 + \ldots + \alpha_n)/n}
  = \prod_{i=1}^n W_i^{\beta_i}
  \end{align*}
  \] where \(\beta_i = \frac{\alpha_1 +\cdots +\alpha_i}{i}.\) The
  expectation is then \[
  \mathbb{E}\left[\prod_{i=1}^n U_{(i)}^{\alpha_i}\right] = \prod_{i=1}^n \mathbb{E}\left[W_i^{\beta_i}\right] = \prod_{i=1}^n (1+\beta_i)^{-1}.
  \]\\
\item
  \[\begin{align*}
  \mathrm{Cov}\left(\log U_{(i)}, \log U_{(j)}\right) &= \mathrm{Cov}\left(-\log U_{(i)}, -\log U_{(j)}\right)\\
  &= \mathrm{Cov}\left(Y_{(n-i+1)},Y_{n-j+1}\right)\\
  &= \mathrm{Cov}\left( \sum_{k=1}^{n-i+1} \frac{1}{n-k+1}X_k, \sum_{k=1}^{n-j+1} \frac{1}{n-k+1} X_k\right)\\
  &= \sum_{k=1}^{n-j+1} \left(\frac{1}{n-k+1}\right)^2 \mathrm{Var}(X_k)\\
  &=  \sum_{k=1}^{n-j+1} \left(\frac{1}{n-k+1}\right)^2 = \sum_{k=j}^n \frac{1}{k^2}.
  \end{align*}\]
\end{enumerate}

(e)\[\begin{align*}
U_{(i)}|U_{(j)} &\sim \frac{S_i}{S_{n+1}}\left| \frac{S_j}{S_{n+1}}\right.\\
&= \underbrace{\frac{S_j}{S_{n+1}}}_{\text{constant}} \cdot  \frac{S_i}{S_j} \left\arrowvert \frac{S_j}{S_{n+1}}\right.\\
&=  \underbrace{\frac{S_j}{S_{n+1}}}_{\text{constant}}  \cdot \left( \frac{S_i}{S_j} \left\arrowvert \frac{S_j}{S_{n+1}}\right.\right)\\
&\sim   \underbrace{\frac{S_j}{S_{n+1}}}_{\text{constant}} \cdot \left(\frac{U_{(i)}}{U_{(j)}} \left\arrowvert \frac{U_{j}}{U_{n}} \right.\right) \\
&\sim  \underbrace{\frac{S_j}{S_{n+1}}}_{\text{constant}}  \cdot \mathrm{Beta}(i,j-i).
\end{align*}\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  \[\begin{align*}
  U_{(i)}|\left(U_{(i+1)},\ldots,U_{(n)}\right)
  &\sim U_{(i+1)}\frac{U_{(i)}}{U_{(i+1)}}\left|\left(U_{(i+1)},\ldots,U_{(n)}\right)\right. \\
  &= U_{(i+1)} \left(\frac{U_{(i)}}{U_{(i+1)}}|\left(U_{(i+1)},\ldots,U_{(n)}\right) \right)\\
  &\sim  U_{(i+1)}   \left(\frac{U_{(i)}}{U_{(i+1)}}\right) \\
  &\sim U_{(i+1)} \mathrm{Beta}(i,1).
  \end{align*}\] The independence
  \(\frac{U_{(i)}}{U_{(i+1)}}\perp\!\!\!\perp \left(U_{(i+1)},\cdots,U_{(n)}\right)\)
  is because
  \[ \frac{U_{(i)}}{U_{(i+1)}}\perp\!\!\!\perp \left(\frac{U_{(i+1)}}{U_{i+2}},\cdots,\frac{U_{(n-1)}}{U_{(n)}},U_{(n)}\right)\]
  and \$ \left(U\_\{(i+1)\},\cdots,U\_\{(n)\}\right) \$ is a function of
  \$
  \left(\frac{U_{(i+1)}}{U_{i+2}},\cdots,\frac{U_{(n-1)}}{U_{(n)}},U\_\{(n)\}\right)\$.
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-2}{%
\section*{Next Week}\label{next-week-2}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  Lebesgue Integral
\item
  Convergence Theorems
\end{itemize}

\includegraphics{./assets/img/lebesgue.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-4}{%
\chapter*{Section 4}\label{section-4}}
\addcontentsline{toc}{chapter}{Section 4}

\markboth{Section 4}{Section 4}

Last Updated: 06 Oct 2023

Date: 06 Oct 2023

\hypertarget{introduction-3}{%
\section*{Introduction}\label{introduction-3}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  Lebesgue Integral
\item
  Convergence Theorems
\end{itemize}

\hypertarget{lebesgue-integral}{%
\section*{Lebesgue Integral}\label{lebesgue-integral}}
\addcontentsline{toc}{section}{Lebesgue Integral}

\markright{Lebesgue Integral}

\leavevmode\vadjust pre{\hypertarget{def-Riemann-Stieltjes-integral}{}}%
\begin{definition}[]\label{def-Riemann-Stieltjes-integral}

(Riemann-Stieltjes integrals)
\(E(X) = \int_{-\infty}^{\infty} x dF(x)\), easier to compute.

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-Lebesgue-integral}{}}%
\begin{definition}[]\label{def-Lebesgue-integral}

(Lebesgue integral) \(E(X) = \int_{\Omega} X(\omega) P(d\omega)\),
easier for proofs.

\end{definition}

To have a unified definition of expected values, and for proving results
about expected values, it is helpful to define \(E\) in stages for
increasingly more general r.v.s. This parallels the construction of the
Lebesgue integral.

\leavevmode\vadjust pre{\hypertarget{def-InSiPoD}{}}%
\begin{definition}[]\label{def-InSiPoD}

(InSiPoD) Let \((\Omega, \mathcal{F}, P)\) be a probability space, and
let \(X: \Omega \rightarrow \mathbb{R}\) be a random variable. Then the
expected value of \(X\), denoted \(\mathbf{E}[X]\) is defined by the
following three-step construction: \({ }^{14}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For indicator random variables, which are simply 1 on a bounded
  measurable set \(S \in \mathcal{F}\) and 0 otherwise. Their
  expectation is the measure \(P(S)\).
\item
  Extending to non-negative weighted sums of indicator random variables,
  called simple random variables. We do this by linearity of
  expectation.
\item
  Defining for non-negative random variables by taking the supremum over
  all dominated simple random variables \(X^*\), \[
  \mathbf{E}[X]=\sup _{X^* \leq X} \mathbf{E}\left[X^*\right] .
  \]
\item
  Extending to general signed random variables by taking a partition
  \(X=X^{+}-X^{-}\)into positive and negative parts, and computing the
  integral for each separately.
\end{enumerate}

\end{definition}

\hypertarget{convergence-theorems}{%
\section*{Convergence Theorems}\label{convergence-theorems}}
\addcontentsline{toc}{section}{Convergence Theorems}

\markright{Convergence Theorems}

\leavevmode\vadjust pre{\hypertarget{def-Almost-sure-convergence}{}}%
\begin{definition}[]\label{def-Almost-sure-convergence}

(Almost sure convergence): \(X_n \xrightarrow{a.s.}X\) if
\(P(\{\omega: X_n(\omega) \xrightarrow{n\to \infty} X(\omega)\}) = 1\)

\end{definition}

\leavevmode\vadjust pre{\hypertarget{thm-Monotone-Convergence-Theorem}{}}%
\begin{theorem}[]\label{thm-Monotone-Convergence-Theorem}

(Monotone Convergence Theorem): Let \(0 \leq X_1 \leq X_2 \leq ...\) be
an \textit{increasing sequence of non-negative} random variables such
that \(X_n \xrightarrow{a.s.} X\) as \(n\to \infty\). Then
\(\lim\limits_{n\to \infty}E(X_n) = E(X)\).

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{def-Fatou-Lemma}{}}%
\begin{definition}[]\label{def-Fatou-Lemma}

(Fatou's Lemma): Let \(X_1,X_2,...\) be a sequence of
\textit{non-negative} random variables. Then
\[E\Big(  \liminf_{n\to \infty} X_n \Big) \leq \liminf_{n\to \infty} E(X_n)\]

\end{definition}

\leavevmode\vadjust pre{\hypertarget{thm-Dominated-Convergence-Theorem}{}}%
\begin{theorem}[]\label{thm-Dominated-Convergence-Theorem}

(Dominated Convergence Theorem): Let \(X_1,X_2,...\) be a sequence of
random variables such that \(X_n \xrightarrow{a.s.} X\) as
\(n\to \infty\). If there exists some r.v \(W\geq 0\) with
\(E(W)<\infty\) such that \(|X_n|\leq W\) for all \(n\geq 1\), then
\(\lim\limits_{n\to \infty}E(X_n) = E(X)\).

\end{theorem}

\leavevmode\vadjust pre{\hypertarget{thm-Bounded-Convergence-Theorem}{}}%
\begin{theorem}[]\label{thm-Bounded-Convergence-Theorem}

(Bounded Convergence Theorem): It's a special case of DCT. If
\(X_n \xrightarrow{a.s.} X\) as \(n\to \infty\) and \(|X_n|\leq c\) for
all \(n\geq 1\) (for a constant \(c\)), then
\(\lim\limits_{n\to \infty}E(X_n) = E(X)\).

\end{theorem}

Monotone Convergence Theorem, Dominated Convergence Theorem (and hence
Bounded Convergence Theorem) also hold when \(X_n \to X\)
\textit{in probability} as \(n\) goes to infinity.

\hypertarget{section-discussion-questions-2}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions-2}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

\hypertarget{section-problem-1-3}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1-3}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

Let \(X\) and \(Y\) be r.v.s with finite variances, on the same
probability space.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Suppose (for this part only) that \(E(Y \mid X)=g(X)\), with
  \(g: \mathbb{R} \rightarrow \mathbb{R}\) an increasing function. Show
  that \(\operatorname{Cov}(X, Y) \geq 0\).
\item
  Suppose that \(E(Y \mid X)=X\) and \(E(X \mid Y)=Y\). Show that
  \(P(X=Y)=1\).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  By
  \(\mathrm{ECCE}, \operatorname{Cov}(X, Y)=E \operatorname{Cov}(X, Y \mid X)+\operatorname{Cov}(E(X \mid X), E(Y \mid X))=\operatorname{Cov}(X, g(X))\).
  By the covariance inequality (proven in class), this is nonnegative.
\item
  We will show \((X-Y)^2=0\) a.s. by showing that \(E(X-Y)^2=0\). By
  Adam's Law, \[
  E(X-Y)^2=E\left(E\left(X^2 \mid X\right)-2 E(X Y \mid X)+E\left(Y^2 \mid X\right)\right)=E\left(Y^2\right)-E\left(X^2\right),
  \] since \(E(X Y \mid X)=X E(Y \mid X)=X^2\). By Adam's Law (now
  conditioning on \(Y\) ), \[
  E(X-Y)^2=E\left(X^2\right)-E\left(Y^2\right) \text {. }
  \] So \(E(X-Y)^2\) is its own negative, which implies \(E(X-Y)^2=0\).
  Thus, \(X=Y\) a.s.
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-3}{%
\subsection*{✏️ Section Problem 2}\label{section-problem-2-3}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2}

Show that for any r.v.s \(X\) and \(Y\) with Var\((Y)\) finite, we have
\[E(Y|E(Y|X))=E(Y|X)\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(g(X)=E(Y|X)\). By Adam's law, \[
\begin{align*}
E(Y|g(X)) &= E(E(Y|X, g(X))|g(X))\\
&= E(E(Y|X)|g(X))\\
&= E(g(X)|g(X))\\
&= g(X)
\end{align*}
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-3}{%
\subsection*{✏️ Section Problem 3}\label{section-problem-3-3}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3}

Let \(X_1, X_2\) and \(Y\) be random variable, \(E(Y^2)\) finite. Let
\[A=E(Y|X_1) \text{ and } B=E(Y|X_1,X_2)\] Show that
\[Var(A) \leq Var(B)\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By Eve's Law, \[Var(B) \geq Var(E(B|X_1))\]

By Adam's Law, \[E(B|X_1)=E(E(Y|X_1, X_2)|X_1)=E(Y|X_1)=A\]

Thus, \[Var(B)\geq Var(E(B|X_1))=Var(A).\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-4-3}{%
\subsection*{✏️ Section Problem 4}\label{section-problem-4-3}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 4}

Let \(Y\) be a nonnegative \(r.v.\) with \(E(Y)< \infty\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Prove that \(E(Y) =\int_0^\infty P(Y > y)dy\),using the Lebesgue
  definition of \(E(Y)\) (keeping in mind that \(Y\) may not be purely
  discrete or purely absolutely continuous). Feel free to use the fact
  that Monotone Convergence, which is shown in the book for expectation,
  also holds for integration over \((0,\infty)\)
\item
  Prove the same result using the Riemann-Stieltjes definition of
  \(E(Y)\). Feel free to assume that you can swap the order of
  integration in a double integral you may encounter (formal
  justification of such swaps is usually done using theorems of Fubini
  and Tonelli).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  We will use InSiPoD (actually InSiPo since \(Y\) is nonnegative). If
  \(Y=I_A\) is an indicator r.v., then
  \(\int_0^\infty P(Y>y)dy = \int_0^1 P(Y=1)dy = P(A)=E(Y).\) Let
  \(Y=\sum_{j=1}^n a_j I_{A_j}\), with the \(a_j\)'s distinct and the
  \(A_j\)'s a partition of \(\Omega\). Then \[
      \begin{align*} 
      \int_0^\infty P(Y>y)dy  &= \int_0^\infty \sum_{j=1}^n P(Y>y|A_j)P(A_j) dy \\
      &= \int_0^\infty \sum_{j=1}^n P(a_j>y|A_j)P(A_j)dy  \\
      &=      \sum_{j=1}^n \int_0^\infty P(a_j>y|A_j)P(A_j) dy \\
      &=    \sum_{j=1}^n a_j P(A_j)  = E(Y).
      \end{align*} 
  \] Now let \(Y\) be a nonnegative r.v., and write
  \(Y = \lim_{n \to \infty} Y_n\), with the \(Y_n\) simple r.v.s and
  \(0 \leq Y_1 \leq Y_2 \leq \dots.\) By Monotone Convergence and
  continuity of probability,
  \[E(Y) = \lim_{n \to \infty} E(Y_n) = \lim_{n \to \infty} \int_0^\infty P(Y_n>y)dy
      = \int_0^\infty \lim_{n \to \infty} P(Y_n>y)dy
      =  \int_0^\infty  P(Y>y)dy.\]
\item
  Let \(F\) be the CDF of \(Y\). Then
  \[ \int_0^\infty P(Y>y)dy = \int_0^\infty \int_y^\infty  dF(x) dy = \int_0^\infty \int_0^x  dy dF(x) = \int_0^\infty x dF(x) = E(Y).\]
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-5-3}{%
\subsection*{✏️ Section Problem 5}\label{section-problem-5-3}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 5}

Let \(X\) be a positive valued random variable with finite expectation.
Show that:
\[\frac{d}{dt}E(e^{-tX}) = - E(Xe^{-tX}) ; \hspace{0.2cm}t>0\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By the sequential definition of derivative, we have: \[
    \begin{align*}
    \frac{d}{dt}E(e^{-tX}) & = \lim\limits_{n\to \infty} \frac{E(e^{-(t+\epsilon_n)X}) - E(e^{-tX})}{\epsilon_n} \hspace{0.2cm} \text{(where $\epsilon_n \xrightarrow{n\to \infty} 0$)}\\
    & = \lim\limits_{n\to \infty} E\Big( \frac{e^{-(t+\epsilon_n)X} - e^{-tX}}{\epsilon_n} \Big) = \lim\limits_{n\to \infty} E(Y_n)
    \end{align*}
\] , where \(Y_n = \frac{e^{-(t+\epsilon_n)X} - e^{-tX}}{\epsilon_n}\)
for \(n\geq 1\).\textbackslash{} Firstly, let
\((\Omega, \mathcal{F}, \mathbb{P})\) be the underlying probability
space. Now, for any \(\omega \in \Omega\), using the sequential
definition of derivative once again we get:
\[\lim\limits_{n\to \infty} Y_n(\omega) = \frac{d}{dt} e^{-tX(\omega)} = -X(\omega)e^{-tX(\omega)}\]
Thus, \(Y_n\) converges to \(-Xe^{-tX}\) point-wise and hence almost
surely, as \(n\to \infty\).

Next we will try to uniformly bound the \(Y_n\)s. We notice that, for
any \(n\geq 1\) \[
    \begin{align*}
    |Y_n| &= \Big |\frac{e^{-(t+\epsilon_n)X} - e^{-tX}}{\epsilon_n}\Big | \\
    & = |e^{-tX}| \Big |\frac{e^{-\epsilon_n X} - 1}{\epsilon_n X}\Big | |X|\\
    & \leq \Big |\frac{e^{-\epsilon_n X} - 1}{\epsilon_n X}\Big | |X| \hspace{0.3cm} \text{($\because$ $X$ is positive valued and $t>0$)}\\
    & = \Big| \frac{f(\epsilon_n X) - f(0)}{\epsilon_n X - 0}  \Big| |X| \hspace{0.3cm} \text{(where $f(u) = e^{-u}$, $u\geq0$})\\
    & = |f'(Z_n)||X| \hspace{0.3cm} \text{(using mean value theorem, where $0<Z_n<\epsilon_n X$)}\\
    & = |-e^{-Z_n}||X| \leq 1 \times |X| = X
    \end{align*}
\]

Furthermore \(E(X)<\infty\). Using Dominated Convergence Theorem, we get
\(\lim\limits_{n\to \infty}E(Y_n) = E(-Xe^{-tX})\). Therefore,
\[ \frac{d}{dt}E(e^{-tX}) = - E(Xe^{-tX}) = E(\frac{d}{dt}e^{-tX})\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-3}{%
\section*{Next Week}\label{next-week-3}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  Conditional Expectation
\item
  Moment Generating Function
\end{itemize}

\includegraphics{./assets/img/mgf.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-5}{%
\chapter*{Section 5}\label{section-5}}
\addcontentsline{toc}{chapter}{Section 5}

\markboth{Section 5}{Section 5}

Last Updated: 13 Oct 2023

Date: 12 Oct 2023

\hypertarget{introduction-4}{%
\section*{Introduction}\label{introduction-4}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  All about Expectation
\end{itemize}

\hypertarget{expectation}{%
\section*{Expectation}\label{expectation}}
\addcontentsline{toc}{section}{Expectation}

\markright{Expectation}

\leavevmode\vadjust pre{\hypertarget{def-expectation}{}}%
\begin{definition}[]\label{def-expectation}

(Expectation) For a discrete random variable \(X\) taking on values
\(x_1, x_2, \cdots\), \[E(X)=\sum_{j=1}^{\infty} x_i P(X=x_i).\] For a
continuous random variable, the expectation \(E(X)\) is given by the
Riemann Intergral \[E(X)= \int_{-\infty}^{\infty} xf(x) dx\]

\end{definition}

To have a unified definition of expected values, and for proving results
about expected values, it is helpful to define \(E\) in stages for
increasingly more general r.v.s. This parallels the construction of the
Lebesgue integral.

\leavevmode\vadjust pre{\hypertarget{def-InSiPoD}{}}%
\begin{definition}[]\label{def-InSiPoD}

(InSiPoD) Let \((\Omega, \mathcal{F}, P)\) be a probability space, and
let \(X: \Omega \rightarrow \mathbb{R}\) be a random variable. Then the
expected value of \(X\), denoted \(\mathbf{E}[X]\) is defined by the
following three-step construction: \({ }^{14}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For indicator random variables, which are simply 1 on a bounded
  measurable set \(S \in \mathcal{F}\) and 0 otherwise. Their
  expectation is the measure \(P(S)\).
\item
  Extending to non-negative weighted sums of indicator random variables,
  called simple random variables. We do this by linearity of
  expectation.
\item
  Defining for non-negative random variables by taking the supremum over
  all dominated simple random variables \(X^*\), \[
  \mathbf{E}[X]=\sup _{X^* \leq X} \mathbf{E}\left[X^*\right] .
  \]
\item
  Extending to general signed random variables by taking a partition
  \(X=X^{+}-X^{-}\)into positive and negative parts, and computing the
  integral for each separately.
\end{enumerate}

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-linearity}{}}%
\begin{definition}[]\label{def-linearity}

(Linearity of Expectation) Expectation \(E\) is linear: for any r.v.s
\(X\) and \(Y\), \[E(X+Y)=E(X)+E(Y).\]

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-fatou-lemma}{}}%
\begin{definition}[]\label{def-fatou-lemma}

(Fatou's Lemma) Let \(X_1, X_2, \cdots\) ve nonnegative r.v.s. Then
\[E(\lim \inf_{n\rightarrow\infty}X_n) \leq \lim \inf_{n\rightarrow\infty}EX_n\]

\end{definition}

\hypertarget{variance-covariance-and-correlation}{%
\section*{Variance, Covariance, and
Correlation}\label{variance-covariance-and-correlation}}
\addcontentsline{toc}{section}{Variance, Covariance, and Correlation}

\markright{Variance, Covariance, and Correlation}

\leavevmode\vadjust pre{\hypertarget{def-variance-std}{}}%
\begin{definition}[]\label{def-variance-std}

(Variance and Standard deviation). The \emph{variance} of \(X\) is
\[Var(X)\equiv E((X-E(X))^2),\] and the \emph{standard deviation} is
simply denoted as \(SD(X)=\sqrt(Var(X))\).

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-cov-corr}{}}%
\begin{definition}[]\label{def-cov-corr}

(Covariance and Correlation). The \emph{covariance} of \(X\) and \(Y\)
is \[Cov(X,Y)\equiv E((X-E(X))(Y-E(Y))).\] and \emph{correlation} is
defined as
\[Cor(X,Y)\equiv Cov \left( \frac{X-EX}{SD(X)},\frac{Y-EY}{SD(Y)} \right)\]

\end{definition}

\hypertarget{adam-eve-and-eece}{%
\section*{Adam, Eve and EECE}\label{adam-eve-and-eece}}
\addcontentsline{toc}{section}{Adam, Eve and EECE}

\markright{Adam, Eve and EECE}

\leavevmode\vadjust pre{\hypertarget{def-adam}{}}%
\begin{definition}[]\label{def-adam}

(Adam's Law) We denote Adam's law in Stat 110 and Stat 210 as
\[E(E(Y|X))=E(Y)\]

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-conditional-variance}{}}%
\begin{definition}[]\label{def-conditional-variance}

(Conditional Variance) Conditional Variance can be defined as
\[Var(Y|X)=E((Y-E(Y|X))^2|X)=E(Y^2|X)-E^2(Y|X)\]

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-eve}{}}%
\begin{definition}[]\label{def-eve}

(Eve's Law) We denote Eve's law in Stat 110 and Stat 210 as
\[Var(Y)=E(Var(Y|X))+Var(E(Y|X)).\]

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-ecce}{}}%
\begin{definition}[]\label{def-ecce}

(ECCE) ECCE is a common term that describe
\[Cov(Y_1, Y_2)=E(Cov(Y_1,Y_2|X))+Cov(E(Y_1|X, E(Y_2|X))\]

\end{definition}

\hypertarget{section-discussion-questions-3}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions-3}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

\hypertarget{section-problem-1-4}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1-4}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

Let \(X_1, X_2\) and \(Y\) be random variable, \(E(Y^2)\) finite. Let
\[A=E(Y|X_1) \text{ and } B=E(Y|X_1,X_2)\] Show that
\[Var(A) \leq Var(B)\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By Eve's Law, \[Var(B) \geq Var(E(B|X_1))\]

By Adam's Law, \[E(B|X_1)=E(E(Y|X_1, X_2)|X_1)=E(Y|X_1)=A\]

Thus, \[Var(B)\geq Var(E(B|X_1))=Var(A).\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-4}{%
\subsection*{✏️ Section Problem 2}\label{section-problem-2-4}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2}

Let \(X, Y, Z\) be random variables with finite variances, such that \[
E(Y \mid X, Z)=\beta_0+\beta_1 X+\beta_2 Z, E(Z \mid X)=\alpha_0+\alpha_1 X .
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Find a fully simplified expression for \(E(Y \mid X)\).
\item
  Assuming for this part only that \(X\) and \(Z\) are independent,
  exhibit the equality of variances:
  \(\operatorname{Var}(E(Y \mid X, Z))=\operatorname{Var}(E(Y \mid X))+\operatorname{Var}(E(Y \mid Z))\).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  By Proposition 5.7.1 and taking out what's known, we can compute \[
  \begin{aligned}
  E(Y \mid X) & =E(E(Y \mid X, Z) \mid X) \\
  & =E\left(\beta_0+\beta_1 X+\beta_2 Z \mid X\right) \\
  & =\beta_0+\beta_1 X+\beta_2 E(Z \mid X) \\
  & =\beta_0+\beta_1 X+\beta_2\left(\alpha_0+\alpha_1 X\right) \\
  & =\left(\beta_0+\beta_2 \alpha_0\right)+\left(\beta_1+\beta_2 \alpha_1\right) X .
  \end{aligned}
  \]
\item
\item
  By Proposition 5.7.1 and taking out what's known, we can similarly
  compute \[
  \begin{aligned}
  E(Y \mid Z) & =E(E(Y \mid X, Z) \mid Z) \\
  & =E\left(\beta_0+\beta_1 X+\beta_2 Z \mid Z\right) \\
  & =\beta_0+\beta_1 E(X \mid Z)+\beta_2 Z \\
  & =\beta_0+\beta_1 E(X)+\beta_2 Z,
  \end{aligned}
  \] where the last line follows from \(X \perp\!\!\!\!\perp Z\). Then,
  since \(\beta_0+\beta_1 E(X)\) is a constant, \[
  \begin{aligned}
  \operatorname{Var}(E(Y \mid Z)) & =\operatorname{Var}\left(\beta_0+\beta_1 E(X)+\beta_2 Z\right) \\
  & =\operatorname{Var}\left(\beta_2 Z\right) \\
  & =\beta_2^2 \operatorname{Var}(Z) .
  \end{aligned}
  \]
\end{enumerate}

Now, given \(X, \beta_0+\beta_1 X\) is constant, and this very fact,
along with \(X \perp\!\!\!\!\perp Z\), yields \[
\begin{aligned}
\operatorname{Var}(E(Y \mid X, Z) \mid X) & =\operatorname{Var}\left(\beta_0+\beta_1 X+\beta_2 Z \mid X\right) \\
& =\operatorname{Var}\left(\beta_2 Z \mid X\right) \\
& =\beta_2^2 \operatorname{Var}(Z)
\end{aligned}
\]

With the totality of the results above, we can then use Eve's law to
conclude that \[
\begin{aligned}
\operatorname{Var}(E(Y \mid X, Z)) & =\operatorname{Var}(E(E(Y \mid X, Z) \mid X))+E(\operatorname{Var}(E(Y \mid X, Z) \mid X)) \\
& =\operatorname{Var}(E(Y \mid X))+E\left(\beta_2^2 \operatorname{Var}(Z)\right) \\
& =\operatorname{Var}(E(Y \mid X))+\beta_2^2 \operatorname{Var}(Z) \\
& =\operatorname{Var}(E(Y \mid X))+\operatorname{Var}(E(Y \mid Z))
\end{aligned}
\] where we recall that \(E(E(Y \mid X, Z) \mid X)=E(Y \mid X)\) by
Proposition 5.7.1. This is precisely the identity of variances that we
needed to exhibit, so we are done.

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-4}{%
\subsection*{✏️ Section Problem 3}\label{section-problem-3-4}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3}

Consider a linear model \(Y=\beta_0+\beta_1 X_1+\beta_2 X_2+\epsilon\),
where \(E\left(\epsilon \mid X_1, X_2\right)=0\). As background: a
method known as ``partial regression'' studies how \(X_1\) and \(Y\) are
related by plotting the residuals \(Y-E\left(Y \mid X_2\right)\) against
the residuals \(X_1-E\left(X_1 \mid X_2\right)\); understanding the plot
requires knowing how the covariance of these relates to \(\beta_1\).
Show that \[
\operatorname{Cov}\left(Y-E\left(Y \mid X_2\right), X_1-E\left(X_1 \mid X_2\right)\right)=\beta_1 E \operatorname{Var}\left(X_1 \mid X_2\right)
\]

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

We want to find \(\operatorname{Cov}\left(W_1, W_2\right)\), where
\(W_1=Y-E\left(Y \mid X_2\right)\) and \(W_2=X_1-\)
\(E\left(X_1 \mid X_2\right)\). Note that \[
\begin{gathered}
E\left(W_2 \mid X_2\right)=E\left(X_1 \mid X_2\right)-E\left(X_1 \mid X_2\right)=0 \\
E\left(\epsilon \mid X_2\right)=E\left(E\left(\epsilon \mid X_1, X_2\right) \mid X_2\right)=E\left(0 \mid X_2\right)=0 .
\end{gathered}
\] We will use ECCE and repeatedly use the fact that
\(\operatorname{Cov}(Z+c, W)=\operatorname{Cov}(Z, W)\) for any constant
\(c\). \[
\begin{aligned}
\operatorname{Cov}\left(W_1, W_2\right) & =E \operatorname{Cov}\left(W_1, W_2 \mid X_2\right)+\operatorname{Cov}\left(E\left(W_1 \mid X_2\right), E\left(W_2 \mid X_2\right)\right) \\
& =E \operatorname{Cov}\left(W_1, W_2 \mid X_2\right) \\
& =E \operatorname{Cov}\left(Y, X_1 \mid X_2\right) \\
& =E\left(\operatorname{Cov}\left(\beta_1 X_1, X_1 \mid X_2\right)+\operatorname{Cov}\left(\epsilon, X_1 \mid X_2\right)\right) \\
& =\beta_1 E\left(\operatorname{Var}\left(X_1 \mid X_2\right)\right)
\end{aligned}
\] since \[
\operatorname{Cov}\left(\epsilon, X_1 \mid X_2\right)=E\left(\epsilon X_1 \mid X_2\right)=E\left(E\left(\epsilon X_1 \mid X_1, X_2\right) \mid X_2\right)=E\left(X_1 E\left(\epsilon \mid X_1, X_2\right) \mid X_2\right)=0
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-4-4}{%
\subsection*{✏️ Section Problem 4}\label{section-problem-4-4}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 4}

Let \(Y\) be a nonnegative \(r.v.\) with \(E(Y)< \infty\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Prove that \(E(Y) =\int_0^\infty P(Y > y)dy\),using the Lebesgue
  definition of \(E(Y)\) (keeping in mind that \(Y\) may not be purely
  discrete or purely absolutely continuous). Feel free to use the fact
  that Monotone Convergence, which is shown in the book for expectation,
  also holds for integration over \((0,\infty)\)
\item
  Prove the same result using the Riemann-Stieltjes definition of
  \(E(Y)\). Feel free to assume that you can swap the order of
  integration in a double integral you may encounter (formal
  justification of such swaps is usually done using theorems of Fubini
  and Tonelli).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  We will use InSiPoD (actually InSiPo since \(Y\) is nonnegative). If
  \(Y=I_A\) is an indicator r.v., then \$
  \int\_0\^{}\infty P(Y\textgreater y)dy = \int\_0\^{}1 P(Y=1)dy =
  P(A)=E(Y).\$ Let \(Y=\sum_{j=1}^n a_j I_{A_j}\), with the \(a_j\)'s
  distinct and the \(A_j\)'s a partition of \(\Omega\). Then \[
      \begin{align*} 
      \int_0^\infty P(Y>y)dy  &= \int_0^\infty \sum_{j=1}^n P(Y>y|A_j)P(A_j) dy \\
      &= \int_0^\infty \sum_{j=1}^n P(a_j>y|A_j)P(A_j)dy  \\
      &=      \sum_{j=1}^n \int_0^\infty P(a_j>y|A_j)P(A_j) dy \\
      &=    \sum_{j=1}^n a_j P(A_j)  = E(Y).
      \end{align*} 
  \] Now let \(Y\) be a nonnegative r.v., and write
  \(Y = \lim_{n \to \infty} Y_n\), with the \(Y_n\) simple r.v.s and
  \(0 \leq Y_1 \leq Y_2 \leq \dots.\) By Monotone Convergence and
  continuity of probability,
  \[E(Y) = \lim_{n \to \infty} E(Y_n) = \lim_{n \to \infty} \int_0^\infty P(Y_n>y)dy
      = \int_0^\infty \lim_{n \to \infty} P(Y_n>y)dy
      =  \int_0^\infty  P(Y>y)dy.\]
\item
  Let \(F\) be the CDF of \(Y\). Then
  \[ \int_0^\infty P(Y>y)dy = \int_0^\infty \int_y^\infty  dF(x) dy = \int_0^\infty \int_0^x  dy dF(x) = \int_0^\infty x dF(x) = E(Y).\]
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-5-4}{%
\subsection*{✏️ Section Problem 5}\label{section-problem-5-4}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 5}

Let \(X\) and \(Y\) be random variables with
\(\operatorname{Var}(Y)<\infty\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  For this part, suppose that \(X\) and \(Y\) are discrete. Let
  \(A=\{x: P(X=x)>0\}\) and \(B=\{y: P(Y=y)>0\}\). Define the function
  \(g: A \rightarrow \mathbb{R}\) by \[
  g(x)=\sum_{y \in B} y P(Y=y \mid X=x) .
  \] The Stat 110 definition of the conditional expectation
  \(E(Y \mid X)\) is the r.v. \(g(X)\). Show that \(g(X)\) satisfies the
  Stat 210 definition of \(E(Y \mid X)\).
\item
  Let \(\mathcal{G}\) be the \(\sigma\)-algebra generated by \(X\),
  i.e., the set of all events of the form \(X^{-1}(A)\), with \(A\)
  Borel. The most common measure-theoretic definition of \(E(Y \mid X)\)
  is that it is an r.v. \(T\) such that the following two conditions
  hold:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  \(T\) is \(\mathcal{G}\)-measurable, i.e.,
  \(T^{-1}(B) \in \mathcal{G}\) for all Borel \(B\);
\item
  For all \(G \in \mathcal{G}\),
\end{enumerate}

\[
\int_{\Omega} I_G(\omega) Y(\omega) P(d \omega)=\int_{\Omega} I_G(\omega) T(\omega) P(d \omega) .
\]

Let \(g(X)\) satisfy the Stat 210 definition of \(E(Y \mid X)\), with
\(g: \mathbb{R} \rightarrow \mathbb{R}\) a measurable function (i.e.,
\(g^{-1}(B)\) is Borel for all Borel \(B\) ). Show that \(g(X)\)
satisfies conditions (i) and (ii).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Let \(h\) be a bounded, measurable function. We need to show that \[ 
  E((Y-g(X)) h(X))=0 .
  \] By linearity, it is equivalent to show that \[
  E(Y h(X))=E(g(X) h(X)) .
  \] By LOTUS, the left-hand side is \[
  \sum_{x \in A} \sum_{y \in B} y h(x) P(X=x, Y=y)
  \] and the right-hand side is \[
  \sum_{x \in A} g(x) h(x) P(X=x) .
  \] Plugging in the formula for \(g(x)\), the right-hand side is \[
  \sum_{x \in A}\left(\sum_{y \in B} y P(Y=y \mid X=x)\right) h(x) P(X=x)=\sum_{x \in A} \sum_{y \in B} y h(x) P(X=x, Y=y),
  \] since we can bring \(h(x) P(X=x)\) inside the sum over \(y\) and
  then use the fact that \[
  P(X=x, Y=y)=P(X=x) P(Y=y \mid X=x) .
  \]
\item
  Let \(T=g(X)\). Recall that, formally, \(T\) is a composition of
  functions: first do \(X\), then do \(g\). For any Borel set \(B\), \[
  T^{-1}(B)=X^{-1}\left(g^{-1}(B)\right) .
  \] Let \(A=g^{-1}(B)\). Then \(A\) is Borel (since \(B\) is Borel and
  \(g\) is a measurable function), so \(T^{-1}(B) \in \mathcal{G}\)
  (since it is of the form \(X^{-1}(A)\) with \(A\) Borel).
\end{enumerate}

Now fix \(G=X^{-1}(C) \in \mathcal{G}\), where \(C\) is Borel. In
expectation notation, we need to verify that \[
E\left(I_G Y\right)=E\left(I_G T\right) .
\] Equivalently, we can show \[
E\left((Y-T) I_G\right)=0 .
\] If we can show that \(I_G\) is of the form \(h(X)\) where \(h\) is a
bounded, measurable function, then we know from the Stat 210 definition
of conditional expectation that the above equation holds. Let \[
h(x)=I(x \in C) .
\] Then \(h\) is bounded (since \(h(x) \in\{0,1\}\) for all \(x\) ) and
measurable (since the preimage of any Borel set is either
\(\emptyset, C, C^c\), or \(\mathbb{R}\) ), and, since \(G\) is the
event \(X \in C\), \[
h(X)=I(X \in C)=I_G .
\] Hence, \[
E\left((Y-T) I_G\right)=0
\] as desired.

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-4}{%
\section*{Next Week}\label{next-week-4}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  Moment Generating Function
\item
  Midterm Review
\end{itemize}

\includegraphics{./assets/img/midterm.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-6}{%
\chapter*{Section 6}\label{section-6}}
\addcontentsline{toc}{chapter}{Section 6}

\markboth{Section 6}{Section 6}

Last Updated: 20 Oct 2023

Date: 20 Oct 2023

\hypertarget{introduction-5}{%
\section*{Introduction}\label{introduction-5}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  Moment Generating Functions
\item
  Midterm
\end{itemize}

\hypertarget{moment-generating-function}{%
\section*{Moment Generating Function}\label{moment-generating-function}}
\addcontentsline{toc}{section}{Moment Generating Function}

\markright{Moment Generating Function}

MGFs are powerful because: (1) Determining distirbutions (2) Powerful
tool for convolution (Theorem 5.4.1) and (3) obtain moments.

\leavevmode\vadjust pre{\hypertarget{def-Moment-Generating-Function}{}}%
\begin{definition}[]\label{def-Moment-Generating-Function}

(Moment Generating Function) We say that a r.v. \(\displaystyle X\) has
a moment generaing function (MGF) if the function
\(\displaystyle M( t) \equiv E\left( e^{tX}\right)\) is finite in an
open interval containing 0.

\end{definition}

If the MGF of \(\displaystyle X\) exists, then all the moments
\(\displaystyle E\left( X^{r}\right)\) exists, but the converse is not
true.

💌 Tips from example 6.1.6 (Normal MGF): If \(Y\) is normal, then
\(E(e^Y)\) is the exponential of the mean of \(Y\) plus half the
variance of \(Y\).

\leavevmode\vadjust pre{\hypertarget{def-n-th-moment}{}}%
\begin{definition}[]\label{def-n-th-moment}

Let \(\displaystyle X\) have an MGF \(\displaystyle M( t)\). Then the
\(\displaystyle n\)th moment of \(\displaystyle X\) is given by
\(\displaystyle E\left( X^{n}\right) =M^{( n)}( 0)\), the
\(\displaystyle n\)th derivative of \(\displaystyle M\) at
\(\displaystyle 0\). Furthermore, in some neighborhood of
\(\displaystyle 0\) the Taylor expansion of \(\displaystyle M\) is
\begin{equation*}
M( t) =\sum _{n=0}^{\infty }\frac{E\left( X^{n}\right)}{n!} t^{n}
\end{equation*}

\end{definition}

Remark 6.1.10 The expression
\(\displaystyle M^{n}( t) =E\left( X^{2} e^{tX}\right)\) immediately
implies that MGFs are convex.

💌 Tips: If the MGF does not exist, replace \(\displaystyle t\) by
\(\displaystyle it\) where \(\displaystyle i=\sqrt{-1}\), this will
yields the characterisitic function (which exists for any r.v). If the
MGF does exist, it determine the distribution.

\leavevmode\vadjust pre{\hypertarget{thm-linearity}{}}%
\begin{theorem}[]\label{thm-linearity}

(Uniqueness and MGF) Let \(\displaystyle X_{1}\) and
\(\displaystyle X_{2}\) have the same MGF \(\displaystyle M( t)\) in
some neighborhood of 0, i.e.,
\(\displaystyle E\left( e^{tX_{1}}\right) =M( t) =E\left( e^{tX_{2}}\right)\)
for \(\displaystyle |t|< c\), where \(\displaystyle c >0\). Then
\(\displaystyle X_{1} \sim X_{2}\).

\end{theorem}

A moment generating function uniquely determines a distribution, but
this does not imply that if \(\displaystyle X_{1}\) and
\(\displaystyle X_{2}\) have exactly the same \(\displaystyle n\)th
moments for all
\(\displaystyle n\)\(\displaystyle \in \{1,2,3,\ \cdots \}\), then
\(\displaystyle X_{1} \sim X_{2} .\)

💌 Tips from Proposition 6.1.14: Let \(\displaystyle X_{1}\),
\(\displaystyle X_{2}\) be independent MGFs with
\(\displaystyle M_{1}( t) ,\ M_{2}( t)\). Then
\(\displaystyle X_{1} +X_{2}\) has MGF
\(\displaystyle M_{1}( t) M_{2}( t)\).

\leavevmode\vadjust pre{\hypertarget{def-kurtosis}{}}%
\begin{definition}[]\label{def-kurtosis}

(Kurtosis) Suppose that \(\displaystyle X\) has a MGF
\(\displaystyle M( t)\). Then the cumulant generating function (CGF) is
\(\displaystyle K( t) \equiv \log M( t)\). Expanding
\(\displaystyle K( t) =\sum _{r=1}^{\infty } \kappa _{r}\frac{t^{r}}{r!} ,\)the
coefficient \(\displaystyle \kappa _{r}\) is called the
\(\displaystyle r\)th cumulant of \(\displaystyle X\), and is also
denoted by \(\displaystyle K_{r}( X)\).

Cumulatns have many useful properties. Assume that each of
\(\displaystyle X\), \(\displaystyle X_{1} ,\ X_{2}\) has an MGF. Let
\(\displaystyle c\) be any constant. Then,

\begin{itemize}
\item
  \(\displaystyle K_{1}( X+c) =K_{1}( X) +c\) and
  \(\displaystyle K_{r}( X+c) =K_{r}( X)\) for all
  \(\displaystyle r\geqslant 2\).
\item
  \(\displaystyle K_{r}( cX) =c^{r} K_{r}( X)\)
\item
  \(\displaystyle K_{r}( X_{1} +X_{2}) =K_{r}( X_{1}) +K_{r}( X_{2})\)
  for \(\displaystyle X_{1} \perp\!\!\!\!\perp X_{2}\).
\item
  \(\displaystyle K_{1}( X)\) is the mean of \(\displaystyle X\),
  \(\displaystyle K_{2}( X)\) is the variance, and
  \(\displaystyle K_{3}( X) =E( E-EX)^{3}\) is the third central moment.
  Standardizing this by dividing by \(\displaystyle Var^{3/2}( X)\)
  yields the skewness of \(\displaystyle X\).
\item
  \(\displaystyle K_{4}( X) =E( X-EX)^{4} -3Var( X))^{2}\).
  Standardizing this by dividing by \(\displaystyle Var^{2}( X)\) yields
  the kurtosis, denoted by Kurt(X).
\end{itemize}

\end{definition}

\hypertarget{midterm-concepts}{%
\section*{Midterm Concepts}\label{midterm-concepts}}
\addcontentsline{toc}{section}{Midterm Concepts}

\markright{Midterm Concepts}

It is redundant to retype all the concepts The following are the
highlighted midterm topics:

\begin{itemize}
\item
  \textbf{Meaning of measure} \protect\hyperlink{section-1}{Section 1}:
  axioms of probability, random variables and their distributions,
  continuity of probability, preimages, independence, \(\pi-\lambda\)
  Theorem
\item
  \textbf{Reasoning by representation} {[}Section 2 + 3{]}: stories and
  representations for the most fundamental distributions, Probability
  Integral Transform, Count-Time Duality, Poisson process (on the line
  or in n dimensions), order statistics
\item
  \textbf{Meaning of means} {[}Section 4 + 5{]}: expected value via the
  Lebesgue integral and via the Riemann--Stieltjes integral, InSiPoD,
  LOTUS, linearity, variance, covariance, Monotone Convergence Theorem,
  Dominated Convergence Theorem, Bounded Convergence Theorem, Darth
  Vader Rule
\item
  \textbf{Conditioning} {[}Section 4 + 5{]}: conditional distributions,
  conditional expectation, Adam's Law (Law of Total Expectation), Eve's
  Law (Law of Total Variance), ECCE (Law of Total Covariance).
\item
  \textbf{Generating functions} \protect\hyperlink{section-6}{Section
  6}: moments and moment generating functions (MGFs), characteristic
  functions, cumulants and cumulant generating functions (CGFs)
\end{itemize}

\hypertarget{section-discussion-questions-4}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions-4}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

This week, as midterm is near, we will discuss fully Midterm 2022 \&
2010, to give a sense on how to approach and think about problems!

\hypertarget{section-problem-1-q1-2022}{%
\subsection*{✏️ Section Problem 1 (Q1,
2022)}\label{section-problem-1-q1-2022}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1 (Q1, 2022)}

Let \((\Omega, \mathcal{F}, P)\) be a probability space and
\(\mathcal{S}\) be a \(\pi\)-system such that \(\Omega \in \mathcal{S}\)
and \(\sigma(\mathcal{S})=\mathcal{F}\). As usual, for any event \(A\),
let \(I(A)\) be the indicator random variable for \(A\). Let
\(\mathcal{V}\) be a collection of random variables on
\((\Omega, \mathcal{F}, P)\) such that:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  If \(X \in \mathcal{V}\) and \(c \in \mathbb{R}\), then
  \(c X \in \mathcal{V}\).
\item
  If \(X, Y \in \mathcal{V}\), then \(X+Y \in \mathcal{V}\).
\item
  If \(X_{1}, X_{2}, \cdots \in \mathcal{V}\) and
  \(\sup _{n} X_{n}(\omega)\) is finite for all \(\omega \in \Omega\),
  then \(\sup _{n} X_{n} \in \mathcal{V}\).
\item
  If \(A \in \mathcal{S}\), then \(I(A) \in \mathcal{V}\).
\end{enumerate}

Show that \(\mathcal{V}\) contains every random variable on
\((\Omega, \mathcal{F}, P)\).

Hint: First consider the set
\(\{A \in \mathcal{F}: I(A) \in \mathcal{V}\}\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

By (i) and (ii), \(\mathcal{V}\) is closed under linear combinations,
i.e., if \(X_{1}, \ldots, X_{n} \in \mathcal{L}\) and
\(c_{1}, \ldots, c_{n} \in \mathbb{R}\), then
\(c_{1} X_{1}+\cdots+c_{n} X_{n} \in \mathcal{L}\). In the spirit of
InSiPoD, we will show that every indicator r.v. is in \(\mathcal{V}\),
then that every simple r.v. is in \(\mathcal{V}\), then that every
nonnegative r.v. is in \(\mathcal{V}\), and lastly that every r.v. is in
\(\mathcal{V}\). Let

\[
\mathcal{L}=\{A \in \mathcal{F}: I(A) \in \mathcal{V}\} .
\]

Then \(\mathcal{L}\) is a \(\lambda\)-system since:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  We have \(\Omega \in \mathcal{L}\) since \(\Omega \in \mathcal{S}\) so
  by (iv), \(I(\Omega) \in \mathcal{V}\).
\item
  If \(A, B \in \mathcal{L}\) with \(A \subseteq B\), then, by closure
  under linear combinations,
\end{enumerate}

\[
I(B \backslash A)=I(B)-I(A) \in \mathcal{V}
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  If \(A_{1}, A_{2}, \cdots \in \mathcal{L}\) with
  \(A_{1} \subseteq A_{2} \subseteq \ldots\), then
  \(A=\bigcup_{n=1}^{\infty} A_{n} \in \mathcal{L}\) by (iii), since
\end{enumerate}

\[
I(A)=\sup _{n} I\left(A_{n}\right)
\]

To check the above equation in more detail, note that if
\(\omega \notin A\) then \(I(A)(\omega)=0\) and
\(I\left(A_{n}\right)(\omega)=0\) for all \(n\), whereas if
\(\omega \in A\), then
\(I(A)(\omega)=1, I\left(A_{n}\right)(\omega) \in\{0,1\}\) for all
\(n\), and \(I\left(A_{n}\right)(\omega)=1\) for all sufficiently large
\(n\).

By (iv), \(\mathcal{S} \subseteq \mathcal{L}\). So by the
\(\pi-\lambda\) theorem, \(\sigma(\mathcal{S}) \subseteq \mathcal{L}\).
But \(\sigma(\mathcal{S})=\mathcal{F}\), so

\[
\mathcal{L}=\mathcal{F}
\]

Thus, all indicator r.v.s are in \(\mathcal{V}\). Every simple r.v. is a
linear combination of indicator r.v.s, so all simple r.v.s are in
\(\mathcal{V}\).

Now let \(X\) be a nonnegative r.v. Write

\[
X=\lim _{n \rightarrow \infty} X_{n}
\]

with the \(X_{n}\) nonnegative simple r.v.s and
\(X_{1} \leq X_{2} \leq \cdots \leq X\). Then \(X=\sup _{n} X_{n}\) (to
check that for each \(\omega, X(\omega)\) is the least upper bound of
\(\left\{X_{1}(\omega), X_{2}(\omega), \ldots\right\}\), note that
\(X_{n}(\omega) \leq X(\omega)\) for all \(n\), and if
\(X_{n}(\omega) \leq c\) for all \(n\), then
\(\lim _{n \rightarrow \infty} X_{n}(\omega) \leq\)
\(\lim _{n \rightarrow \infty} c\), which gives
\(\left.X(\omega) \leq c\right)\).

Thus, by (iii) we have \(X \in \mathcal{V}\). Now let \(X\) be an
arbitrary r.v., and (as usual) let \(X^{+}=\max (X, 0)\) and
\(X^{-}=\max (-X, 0)\). Then \(X^{+}\)and \(X^{-}\)are in
\(\mathcal{V}\) since they are nonnegative r.v.s, so

\[
X=X^{+}-X^{-} \in \mathcal{V}
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-q2-2010}{%
\subsection*{✏️ Section Problem 2 (Q2,
2010)}\label{section-problem-2-q2-2010}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2 (Q2, 2010)}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Let \(Y \sim \mathcal{N}\left(\mu, \sigma^2\right)\), with
  \(\mu \sim \mathcal{N}\left(\mu_0, \sigma^2 / r\right)\). Show by
  representation that \[
  Y \sim \mathcal{N}\left(\mu_0, \sigma^2 \cdot(1+1 / r)\right)
  \]
\item
  Now suppose \(\mu=0\) is known, but \(\sigma^2 \sim t\). Expo. Given
  only one Uniform \(U \sim\) Unif and one independent Normal
  \(Z \sim \mathcal{N}(0,1)\), show how one can easily simulate one draw
  from the marginal distribution of \(Y\). What is the marginal variance
  of \(Y\) ?
\end{enumerate}

Note: avoid using density functions and integrals here; instead, think
through why the results above are ``obvious'' from a representation
perspective. Be careful to justify independence, wherever used.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Let \(Y \sim \mathcal{N}\left(\mu, \sigma^{2}\right)\), with
  \(\mu \sim \mathcal{N}\left(\mu_{0}, \sigma^{2} / r\right)\). Show by
  representation that
\end{enumerate}

\[
Y \sim \mathcal{N}\left(\mu_{0}, \sigma^{2} \cdot(1+1 / r)\right)
\]

By the properties of MVN,

\[
\begin{aligned}
Y \mid \mu & \sim \mathcal{N}\left(\mu, \sigma^{2}\right), \mu \sim \mathcal{N}\left(\mu_{0}, \sigma^{2} / r\right) \\
(Y, \mu) & \sim \mathcal{N}\left(\left(\begin{array}{c}
\mu_{0} \\
\mu_{0}
\end{array}\right),\left(\begin{array}{cc}
\sigma^{2}+\sigma^{2} / r & \sigma^{2} / r \\
\sigma^{2} / r & \sigma^{2} / r
\end{array}\right)\right) \\
Y & \sim \mathcal{N}\left(\mu_{0}, \sigma^{2}(1+1 / r)\right)
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Now suppose \(\mu=0\) is known, but \(\sigma^{2} \sim t\). Expo. Given
  only one Uniform \(U \sim\) Unif and one independent Normal
  \(Z \sim \mathcal{N}(0,1)\), show how one can easily simulate one draw
  from the marginal distribution of \(Y\). What is the marginal variance
  of \(Y\) ?
\end{enumerate}

Note: avoid using density functions and integrals here; instead, think
through why the results above are ``obvious'' from a representation
perspective. Be careful to justify independence, wherever used.

We want to simulate a draw from
\(Y \mid \sigma^{2} \sim \mathcal{N}\left(0, \sigma^{2}\right), \sigma^{2} \sim t\)
Expo. First, we can pick \(\sigma^{2}\) from the distribution
\(-t \log U\), as we are given the Uniform. Second, wecan pick \(Y\)
from \(Y \mid \sigma^{2}\), as we already picked the \(\sigma^{2}\)
according to the Exponential distribution. Then

\[
Y \mid \sigma^{2} \sim \mathcal{N}\left(0, \sigma^{2}\right)
\]

Write \(Y=\sigma Z\). Then \(Z\) is independent of \(\sigma\) since the
conditional distribution of \(Z=Y / \sigma\) is \(\mathcal{N}(0,1)\),
which does not depend on \(\sigma\).

The variance of \(Y\) is

\[
\operatorname{Var}(Y)=\operatorname{Var}\left(E\left(Y \mid \sigma^{2}\right)\right)+E\left(\operatorname{Var}\left(Y \mid \sigma^{2}\right)\right)=E\left(\sigma^{2}\right)=t
\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-q3-2022}{%
\subsection*{✏️ Section Problem 3 (Q3,
2022)}\label{section-problem-3-q3-2022}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3 (Q3, 2022)}

Let \(X_{1}, X_{2}, \ldots\) be i.i.d., with an MGF that exists. Let
\(m_{n}=E\left(X_{1}^{n}\right)\). Let
\(N \sim \operatorname{Pois}(1)\), with \(N\) independent of
\(X_{1}, X_{2}, \ldots\) Let

\[
T=\sum_{j=1}^{N} X_{j}
\]

Find the \(r\) th cumulant of \(T\), for \(r\) a positive integer (in
terms of \(m_{1}, m_{2}, \ldots\) ).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(M\) be the MGF of \(X_{1}\), with \(M(t)<\infty\) for all
\(t \in(-a, a)\). For the rest of this solution, suppose that
\(t \in(-a, a)\). Then

\[
M(t)=\sum_{n=0}^{\infty} \frac{m_{n} t^{n}}{n !}
\]

By Adam's law and LOTUS, the MGF of \(T\) is given by

\[
E\left(E\left(e^{t \sum_{j=1}^{N} X_{j}} \mid N\right)\right)=E\left(M(t)^{N}\right)=e^{-1} \sum_{n=0}^{\infty} \frac{M(t)^{n}}{n !}=e^{M(t)-1}
\]

So the CGF of \(T\) is

\[
M(t)-1=\sum_{n=1}^{\infty} \frac{m_{n} t^{n}}{n !}
\]

The \(r\) th cumulant of \(T\) is the coefficient of \(t^{r} / r\) ! in
the Taylor expansion of the CGF of \(T\) about 0 . Hence, the \(r\) th
cumulant of \(T\) is \(m_{r}\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-5}{%
\section*{Next Week}\label{next-week-5}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  No section since it crashes with midterm. Good luck with your midterm
\end{itemize}

\includegraphics{./assets/img/gluck.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-7}{%
\chapter*{Section 7}\label{section-7}}
\addcontentsline{toc}{chapter}{Section 7}

\markboth{Section 7}{Section 7}

Last Updated: 2 Nov 2023

Date: 3 Nov 2023

\hypertarget{welcome-back}{%
\section*{Welcome Back}\label{welcome-back}}
\addcontentsline{toc}{section}{Welcome Back}

\markright{Welcome Back}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  How was Midterm?
\item
  Multivariate Normal
\end{itemize}

\hypertarget{definition-by-representation}{%
\section*{Definition by
Representation}\label{definition-by-representation}}
\addcontentsline{toc}{section}{Definition by Representation}

\markright{Definition by Representation}

Let's define what is MVN, building from i.i.d of univariate Normals.

\leavevmode\vadjust pre{\hypertarget{def-MVN}{}}%
\begin{definition}[]\label{def-MVN}

(Multivariate Normal) The random vector
\(\mathbf{Y} = (Y_1, Y_2, \cdots, Y_k)\) has the \emph{Multivariate
Normal Distribution} if it is the form
\[\mathbf{Y} = A \mathbf{Z} + \mathbf{\mu}, \] where
\(Z_1, Z_2, \cdots, Z_m\) are i.i.d \(N(0,1)\) random variable, \(A\) is
a \(k\) by \(m\) matrix, and \(\mathbf{\mu} \in \mathbb{R}^k\). We write
\(\mathbf{Y} \sim N_k(\mathbf{\mu}, V)\) if \(\mathbf{Y}\) is a
Multivariate Normal of dimension \(k\) with mean \(\mathbf{\mu}\) and
covariance matrix \(V\).

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-MVN-2}{}}%
\begin{definition}[]\label{def-MVN-2}

(Multivariate Normal by Projections).A random vector \(\mathbf{Y}\) is
multivariate normal if and only if every non zero linear combination of
its components,\(\mathbf{b}^T\mathbf{Y}\) where \(\mathbf{b}=0\),is
distributed according to a univariate normal.

\end{definition}

\hypertarget{properties-about-multivariate-normal}{%
\section*{Properties about Multivariate
Normal}\label{properties-about-multivariate-normal}}
\addcontentsline{toc}{section}{Properties about Multivariate Normal}

\markright{Properties about Multivariate Normal}

\leavevmode\vadjust pre{\hypertarget{thm-moment-MVN}{}}%
\begin{theorem}[]\label{thm-moment-MVN}

(Cramér-Wold device).Given a finite-dimensional random vector
\(\mathbf{X}\), the joint distribution of \(\mathbf{X}\) is uniquely
determined by its projections onto 1-dimensional spaces. In other words,
knowing the marginal distribution of \(\mathbf{t}^T \mathbf{X}\), for
every fixed \(\mathbf{t}\), is enough.

\end{theorem}

The above proposition follows from the fact that joint characteristic
functions determine multivariate distributions (a hard but standard fact
from analysis). This is because the joint characteristic function is
just \[
\varphi_{\mathbf{X}}(\mathbf{t})=\mathbf{E}\left[e^{i \mathbf{t}^T \mathbf{X}}\right] .
\]

In particular, \(\mathbf{t}^T \mathbf{X}\) occurs in the exponent, so
values of the characteristic function are completely determined from
marginal distributions of projections of \(\mathbf{X}\).

\leavevmode\vadjust pre{\hypertarget{def-mgf-mvn}{}}%
\begin{definition}[]\label{def-mgf-mvn}

(MGF of multivariate normal). Recall that the moment generating function
of a univariate \(\mathcal{N}\left(\mu, \sigma^2\right)\) normal
distribution is \[
e^{t \mu+\sigma^2 t^2 / 2}
\]

The joint moment generating function of a multivariate normal
\(\mathcal{N}(\mu, \Sigma)\) is analogously \[
e^{\mathbf{t}^T\left(\mu+\frac{1}{2} \Sigma \mathbf{t}\right)} .
\]

This is because if we let \(\mathbf{W} \sim \mathcal{N}(\mu, \Sigma)\)
and consider the projection \(\mathbf{t}^T \mathbf{W}\), we get
\(\mathbf{E}\left[\mathbf{t}^T \mathbf{W}\right]=\mathbf{t}^T \mu\) and
\(\operatorname{Var}\left[\mathbf{t}^T \mathbf{W}\right]=\operatorname{Var}\left[\mathbf{t}^T\left(\Sigma^{1 / 2} \mathbf{W}+\mu\right)\right]=\operatorname{Var}\left[\mathbf{t}^T \Sigma^{1 / 2} \mathbf{Z}\right]=\mathbf{t}^T \Sigma \mathbf{t}\).
Therefore, the distribution of each projection of a multivariate normal
is also normal, so we can compute the joint MGF from the univariate MGF.

\end{definition}

\leavevmode\vadjust pre{\hypertarget{def-closure}{}}%
\begin{definition}[]\label{def-closure}

(Closure properties of MVN). The multivariate normal distribution has
many nice closure properties, such as:

\begin{itemize}
\item
  If you take a linear combination or shift of multivariate normals, it
  is also multivariate normal.
\item
  Any vector of projections (i.e., projection matrix) is also
  multivariate normal.
\item
  The conditional distribution of a multivariate normal is also
  multivariate normal.
\end{itemize}

\end{definition}

\hypertarget{some-useful-linear-algebra-results}{%
\section*{Some useful linear algebra
results}\label{some-useful-linear-algebra-results}}
\addcontentsline{toc}{section}{Some useful linear algebra results}

\markright{Some useful linear algebra results}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The trace of a matrix is a linear function.
\item
  If \(A\) is \(n\times k\) and \(B\) is \(k\times n\),
  \(\mbox{tr}(AB)=\mbox{tr}(BA)\).
\item
  \textbf{Spectral decomposition}: Suppose \(A_{n\times n}\) is a real
  valued and symmetric with rank \(k\). Then there exists a matrix
  \(\Gamma_{n\times k}=(\gamma_{1},\dots,\gamma_{k})\) with orthogonal
  columns and
  \(\Lambda_{k \times k}=\mbox{{diag}}(\lambda_{1},\dots,\lambda_{k})\)
  with \(\lambda_{i}\neq0\) such that \(A=\Gamma\Lambda\Gamma^{\top}\).
  The \(\gamma_{i}\)s are the eigenvectors of \(A\), with
  \(\lambda_{i}\)s being the corresponding nonzero eigenvalues.
\item
  If \(A\) is non-negative definite it has nonnegative eigenvalues.
\end{enumerate}

\hypertarget{section-discussion-questions-5}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions-5}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

\hypertarget{section-problem-1-5}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1-5}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

Suppose
\(\mathbf{Y}_{n\times 1}\sim \mathcal{N}_n(\mathbf{\mu},\Sigma)\), and
define \(Q=\mathbf{Y}^{\top}A\mathbf{Y}\), where \(A\) is a
\(n \times n\) symmetric matrix.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Find the expectation of \(Q\).
\item
  Find the variance of \(Q\).
\item
  For symmetric matrices \(A_{1}\) and \(A_{2}\) and corresponding
  quadratic forms \(Q_{i}=\mathbf{Y}^{\top}A_{i}\mathbf{Y}\), find the
  covariance of \(Q_{1}\) and \(Q_{2}\).
\item
  How could we have solved this problem if \(A\) was not symmetric?
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  By the linearity and the cyclical properties of the trace \[
  \begin{align*}
  \mathbb{E}[Q] &= \mathbb{E}[\text{tr}(\mathbf{\mu}{Y}^{\top}A\mathbf{\mu}{Y})] =  \mathbb{E}[\text{tr}(A\mathbf{\mu}{Y}\mathbf{\mu}{Y}^{\top})] = \text{tr}[A\mathbb{E}(\mathbf{\mu}{Y}\mathbf{\mu}{Y}^{\top})] \\
  &=  \text{tr}[A(\Sigma + \mathbf{\mu}{\mu}\mathbf{\mu}{\mu}^{\top})] = \text{tr}(A\Sigma) +  \text{tr}(A\mathbf{\mu}{\mu}\mathbf{\mu}{\mu}^{\top}) = \text{tr}(A\Sigma) +  \mathbf{\mu}{\mu}^{\top}A\mathbf{\mu}{\mu}.
  \end{align*}
  \]
\item
  Since \(\Sigma\) is symmetric, we can find a spectral decomposition
  \(\Sigma=\Gamma^{\star} \Lambda \Gamma^{\star \top}\). Since
  \(\Sigma\) is non-negative definite, its non-zero eigenvectors are
  positive. Let us denote
  \(\Sigma^{1 / 2}:=\Gamma^{\star} \Lambda^{1 / 2} \Gamma^{\star \top}\)
  (which is well defined). Note that \(\Sigma^{1 / 2}\) is symmetric and
  \(\Sigma^{1 / 2} \Sigma^{1 / 2}=\Sigma\). Hence, we can represent
  \(\mathbf{Y}=\mu+\Sigma^{1 / 2} \mathbf{Z}\), with
  \(\mathbf{Z} \sim \mathcal{N}_n(0, I)\). Now,
  \(Q=\left(\mu+\Sigma^{1 / 2} \mathbf{Z}\right)^{\top} A\left(\mu+\Sigma^{1 / 2} \mathbf{Z}\right)=\mu^{\top} A \mu+2 \mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}+\mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\).
  \[
  \begin{aligned}
  \operatorname{Var}(Q) & =\operatorname{Var}\left(2 \mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}+\mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\right) \\
  & =4 \underbrace{\operatorname{Var}\left(\mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}\right)}_{(1)}+\underbrace{\operatorname{Var}\left(\mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\right)}_{(2)}+4 \underbrace{\operatorname{Cov}\left(\mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}, \mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\right)}_{(3)} .
  \end{aligned}
  \]
\end{enumerate}

For term (1), we have \[
\operatorname{Var}\left(\mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}\right)=\mu^{\top} A \Sigma^{1 / 2} \operatorname{Var}(\mathbf{Z}) \Sigma^{1 / 2} A \mu=\mu^{\top} A \Sigma A \mu .
\]

For term (2), we use spectral decomposition
\(\Sigma^{1 / 2} A \Sigma^{1 / 2}=\Gamma D \Gamma^{\top}\). Since
\(\Gamma\) is orthogonal, we know \(\Gamma^{\top} \Gamma=I\) and have
\(\tilde{\mathbf{Z}}=\Gamma^{\top} \mathbf{Z} \sim \mathcal{N}(0, I)\).
Since \(D\) is diagonal, we can write
\(\mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}=\)
\(\sum_{i=1}^p d_{i i} \tilde{Z}_i^2\). Using independence, properties
of orthogonal matrices, the cyclical properties of the trace, and the
fact that \(\operatorname{Var}\left(\tilde{Z}_i^2\right)=2\), term
\((2)\) reduces to \[
\begin{aligned}
\operatorname{Var}\left(\sum d_{i i} \tilde{Z}_i^2\right) & =2 \sum d_{i i}^2=2 \operatorname{tr}\left(D^2\right)=2 \operatorname{tr}\left(D^2 \Gamma^{\top} \Gamma\right) \\
& =2 \operatorname{tr}\left(\Gamma D^2 \Gamma^{\top}\right)=2 \operatorname{tr}\left(\left(\Gamma D \Gamma^{\top}\right)^2\right) \\
& =2 \operatorname{tr}\left(\left(\Sigma^{1 / 2} A \Sigma^{1 / 2}\right)^2\right)=2 \operatorname{tr}\left((A \Sigma)^2\right) .
\end{aligned}
\]

For term (3), since \(\mathbf{Z} \sim-\mathbf{Z}\), we can write \[
\begin{aligned}
\operatorname{Cov}\left(\mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}, \mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\right) & =\operatorname{Cov}\left(\mu^{\top} A \Sigma^{1 / 2}(-\mathbf{Z}),\left(-\mathbf{Z}^{\top}\right) \Sigma^{1 / 2} A \Sigma^{1 / 2}(-\mathbf{Z})\right) \\
& =-\operatorname{Cov}\left(\mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}, \mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\right),
\end{aligned}
\] which gives us
\(\operatorname{Cov}\left(\mu^{\top} A \Sigma^{1 / 2} \mathbf{Z}, \mathbf{Z}^{\top} \Sigma^{1 / 2} A \Sigma^{1 / 2} \mathbf{Z}\right)=0\).
The final result is therefore \[
\operatorname{Var}\{Q\}=4 \mu^{\top} A \Sigma A \mu+2 \operatorname{tr}\left((A \Sigma)^2\right)
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\item
  Firstly, note that
  \[\operatorname{Cov}(Q_1,Q_2) = \frac{\operatorname{Var}(Q_1+Q_2) - \operatorname{Var}(Q_1-Q_2)}{4}.\]
  Moreover, \(Q_1+Q_2 = \mathbf{Y}^\top(A_1+A_2)\mathbf{Y}\),
  \(Q_1-Q_2 = \mathbf{Y}^\top(A_1-A_2)\mathbf{Y}\), and \(A_1+A_2\),
  \(A_1-A_2\) are symmetric matrices. We could therefore apply (b) to
  find the covariance.
\item
  The symmetry of \(A\) was not required to find \(\mathbb{E}(Q)\).
  However, we used the symmetry of \(A\) to find
  \(\operatorname{Var}(Q)\). Since \(Q\) is a scalar, we have
  \[Q =  \mathbf{Y}^\top A\mathbf{Y} = (\mathbf{Y}^\top A\mathbf{Y})^\top =  \mathbf{Y}^\top A^\top\mathbf{Y} = \mathbf{Y}^\top\frac{(A+A^\top)}{2}\mathbf{Y}.\]
  Note that \(\frac{(A+A^\top)}{2}\) is symmetric. We can now apply the
  result in (b) to find out \(\operatorname{Var}(Q)\).
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-8.17}{%
\subsection*{✏️ Section Problem 2 (8.17)}\label{section-problem-2-8.17}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2 (8.17)}

(Interclass correlation model) Let \(\mathbf{X}=(X_1, \cdots, X_k)\) be
Multivariate Normal with mean vector \(\mu(1,1,\cdots, 1)\) and
covariance matrix \(\sigma^2 C\), where \(C_{ii}=1\) for all \(i\) and
\(C_{ij}=\rho\) for all \(i\neq j\), for some \(\rho \in (-1,1)\). This
is known as the \emph{interclass correlation model} (and is based on an
exchangeability assumption)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Assume \(k=3\) for this part. Why do we require that
  \(rho \geq -1/2\)?
\item
  Find the joint distribution (with means and covariance) of the vector
  \((\bar{X}, X_1-\bar{X}, X_1-X_2)\).
\item
  Show that \(\sigma^2 C\) has only two distinct eigenvalues (for
  \(\rho \neq 0\)), namely \(\lambda_1 \equiv \sigma^2(1+(k-1)\rho)\)
  and \(\lambda_2 \equiv \sigma^2(1-\rho)\), with multiplicities 1 and
  \(k-1\) respectively.
\end{enumerate}

Hint: Write \(C = (1-\rho)I + \rho J\) in terms of the projection
matrices \(J/k\) and \(I-J/k\), where \(J\) is the matrix of all 1's.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Since
  \(\operatorname{var}\left(X_1+X_2+X_2\right)=\sigma^2(1+1+1+2 \rho+2 \rho+2 \rho) \geq 0\),
  we have \(\rho \geq-1 / 2\).
\item
  Since \(\left(\bar{X}, X_1-\bar{X}, X_1-X_2\right)\) is a linear
  transformation of \(\mathbf{X}\), it follows multivariate normal
  distribution, and we only need to determine its mean vector and
  covariance matrix. By linearity, we have
  \(E\left(\bar{X}, X_1-\bar{X}, X_1-X_2\right)=(\mu, 0,0)\). The
  variances and covariances are \[
  \begin{aligned}
  \operatorname{var}(\bar{X}) & =\sigma^2 / k^2\left[k \sigma^2+k(k-1) \rho\right]=\sigma^2[1+(k-1) \rho] / k \\
  \operatorname{var}\left(X_1-\bar{X}\right) & =\sigma^2 / k[k+1+(k-1) \rho-2\{1+(k-1) \rho\}]=(k-1)(1-\rho) \sigma^2 / k \\
  \operatorname{var}\left(X_1-X_2\right) & =\sigma^2(1+1+2 \rho)=2 \sigma^2(1-\rho) \\
  \operatorname{cov}\left(\bar{X}, X_1-\bar{X}\right) & =\sigma^2(1 / k-1 / k)=0 \\
  \operatorname{cov}\left(\bar{X}, X_1-X_2\right) & =0 \\
  \operatorname{cov}\left(X_1-\bar{X}, X_1-X_2\right) & =\sigma^2(1-\rho)
  \end{aligned}
  \]
\end{enumerate}

Therefore, \[
\left(\begin{array}{c}
\bar{X} \\
X_1-\bar{X} \\
X_1-X_2
\end{array}\right) \sim \mathcal{N}_3\left[\left(\begin{array}{c}
\mu \\
0 \\
0
\end{array}\right), \sigma^2\left(\begin{array}{ccc}
{[1+(k-1) \rho] / k} & 0 & 0 \\
0 & (k-1)(1-\rho) / k & 1-\rho \\
0 & 1-\rho & 2(1-\rho)
\end{array}\right)\right]
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  The equation \(C \alpha=\lambda \alpha\) is equivalent to \[
  (1-\rho) \alpha+\rho J \alpha=\left(\lambda / \sigma^2\right) \alpha
  \] which can be rewritten as \[
  J \alpha=\frac{\frac{\lambda}{\sigma^2}+\rho-1}{\rho} \alpha
  \]
\end{enumerate}

Since \(J \mathbf{1}_k=k \mathbf{1}_k\) and
\(\operatorname{rank}(J)=\operatorname{rank}\left(\mathbf{1}_k \mathbf{1}_k^{\prime}\right)=\operatorname{rank}\left(\mathbf{1}_k\right)=1\),
the matrix \(J\) has only one non-zero eigenvalue equal to \(k\), and
\((k-1)\) zero eigenvalues. From \(\lambda / \sigma^2+\rho-1=k \rho\),
we have \(\lambda=[1+(k-1) \rho] \sigma^2\); and from
\(\lambda / \sigma^2+\rho-1=0\), we have \(\lambda=\sigma^2(1-\rho)\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-8.11}{%
\subsection*{✏️ Section Problem 3 (8.11)}\label{section-problem-3-8.11}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3 (8.11)}

(Matrix times a MVN vector) Let \(Y \sim N_k(\mu, V)\) be Multivariate
Normal and \(X=B Y\), where \(B\) is an \(m\) by \(k\) matrix of
constants, such that \(B V B^{\top}\) is invertible.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Is \((X, Y)\) Multivariate Normal? Explain.
\item
  Find the conditional distribution of \(Y\) given \(X\).
\item
  What does your answer reduce to when \(m=k\) and \(B\) is invertible?
  Does this make sense intuitively?
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Yes. Note that \[
  \begin{aligned}
  \left(\begin{array}{c}
  X \\
  Y
  \end{array}\right) & =\left(\begin{array}{c}
  B \\
  I
  \end{array}\right) Y \\
  & \sim N\left(\left(\begin{array}{c}
  B \\
  I
  \end{array}\right) \mu,\left(\begin{array}{c}
  B \\
  I
  \end{array}\right) V\left(\begin{array}{ll}
  B^{\top} & I
  \end{array}\right)\right) \\
  & =N\left(\left(\begin{array}{c}
  B \mu \\
  \mu
  \end{array}\right),\left(\begin{array}{cc}
  B V B^{\top} & B V \\
  V B^{\top} & V
  \end{array}\right)\right) .
  \end{aligned}
  \]
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(X\) and \(Y\) are marginally multivariate normal doesn't imply that
  \((X, Y)\) is multivariate normal.
\item
  Any linear transformation of any multivariate normal is still a
  multivariate normal (you don't have to start with i.i.d. normals).
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
  By Proposition 8.5.1, \[
  Y \mid X \sim N\left(\mu+V B^{\top}\left(B V B^{\top}\right)^{-1}(X-B \mu), V-V B^{\top}\left(B V B^{\top}\right)^{-1} B V\right)
  \]
\item
  If \(B\) is invertible and \(B V B^{\top}\) is invertible, then \(V\)
  is invertible. Now \[
  \begin{aligned}
  \mu+V B^{\top}\left(B V B^{\top}\right)^{-1}(X-B \mu) & =\mu+V B^{\top}\left(B^{\top}\right)^{-1} V^{-1} B^{-1}(X-B \mu) \\
  & =\mu+B^{-1}(X-B \mu) \\
  & =B^{-1} X . \\
  V-V B^{\top}\left(B V B^{\top}\right)^{-1} B V & =V-V B^{\top}\left(B^{\top}\right)^{-1} V^{-1} B^{-1} B V \\
  & =V-V V^{-1} V=O .
  \end{aligned}
  \]
\end{enumerate}

Hence \(Y \mid X \sim N\left(B^{-1} X, O\right)\). This means that given
\(X, Y\) is a constant. This sure makes sense because if \(B\) is
invertible then \(Y=B^{-1} X\) is a function of \(X\), so knowing \(X\)
is knowing \(Y\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-4-final-2011-q2}{%
\subsection*{✏️ Section Problem 4 (Final 2011,
Q2)}\label{section-problem-4-final-2011-q2}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 4 (Final 2011, Q2)}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Show that within a Multivariate Normal, conditioning on more
  information reduces variance:
  \(\operatorname{Var}\left(Y \mid \mathbf{X}_1\right) \leq \operatorname{Var}\left(Y \mid \mathbf{X}_{\mathbf{2}}\right)\)
  if \(Y, \mathbf{X}_1, \mathbf{X}_{\mathbf{2}}\) are subvectors of a
  MVN random vector, with \(Y\) onedimensional and
  \(\mathbf{X}_{\mathbf{2}}\) a subvector of
  \(\mathbf{X}_{\mathbf{1}}\).
\item
  Give a counterexample to the above if the distribution is not MVN. On
  the other hand, show that on average conditioning on more information
  reduces variance:
  \(E\left(\operatorname{Var}\left(Y \mid \mathbf{X}_1\right)\right) \leq\)
  \(E\left(\operatorname{Var}\left(Y \mid \mathbf{X}_{\mathbf{2}}\right)\right)\).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  SNoTE and Method 1: First show that within a MVN, conditioning reduces
  variance; this is the case where \(\mathbf{X}_{\mathbf{2}}\) is an
  empty vector. For this case, \[
  \operatorname{Var}\left(Y \mid \mathbf{X}_1\right)=V_{22}-V_{21} V_{11}^{-1} V_{12} \leq V_{22}=\operatorname{Var}(Y),
  \] since \(V_{11}\) is positive definite (which implies that
  \(V_{11}^{-1}\) is positive definite). Remarkably, this SNoTE solves
  the general case too, since conditional distributions in a MVN are
  MVN: the conditional distribution of
  \(Y \mid \mathbf{X}_{\mathbf{2}}\) is MVN, and conditioning on any
  additional components in \(\mathbf{X}_1\) reduces the variance.
\end{enumerate}

Method 2: Using the uncorrelation trick, we can write
\(Y=\tilde{Y}+B \mathbf{X}_1\), with \(B\) a constant matrix (a row
vector here) and with \(\tilde{Y}\) independent of \(\mathbf{X}_1\).
Note that \(\tilde{Y}\) and \(\mathbf{X}_1\) are also conditionally
independent given \(\mathbf{X}_2\). So
\(\operatorname{Var}\left(Y \mid \mathbf{X}_1\right)=\operatorname{Var}(\tilde{Y})\)
and \[
\operatorname{Var}\left(Y \mid \mathbf{X}_2\right)=\operatorname{Var}(\tilde{Y})+\operatorname{Var}\left(B \mathbf{X}_{\mathbf{1}} \mid \mathbf{X}_{\mathbf{2}}\right) \geq \operatorname{Var}\left(Y \mid \mathbf{X}_{\mathbf{1}}\right)
\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  For a counterexample, we can choose an example where learning
  something new increases our uncertainty about something else, e.g., if
  you get into your top choice of school then you may have little
  uncertainty about where to go, whereas if you don't get in then you
  may be very uncertain about what to do next. For a simple, precise
  example, let \(Y=W X_1\) where \(X_1 \sim \operatorname{Bern}(1 / 2)\)
  is independent of \(W \sim[0,1000]\). Then
  \(\operatorname{Var}(Y)=E\left(W^2\right) E\left(X_1\right)=500\), but
  \(\operatorname{Var}\left(Y \mid X_1=1\right)=1000\).
\end{enumerate}

Returning to the general case, Eve's Law gives \[
\operatorname{Var}\left(Y \mid \mathbf{X}_{\mathbf{2}}\right)=E\left(\operatorname{Var}\left(Y \mid \mathbf{X}_1\right) \mid \mathbf{X}_{\mathbf{2}}\right)+\operatorname{Var}\left(E\left(Y \mid \mathbf{X}_{\mathbf{1}}\right) \mid \mathbf{X}_{\mathbf{2}}\right) \geq E\left(\operatorname{Var}\left(Y \mid \mathbf{X}_1\right) \mid \mathbf{X}_{\mathbf{2}}\right) .
\]

Taking expectations of both sides, we have \[
E\left(\operatorname{Var}\left(Y \mid \mathbf{X}_2\right)\right) \geq E\left(\operatorname{Var}\left(Y \mid \mathbf{X}_1\right)\right) .
\]

This also gives a third proof of the result from (a), since in an MVN
conditional variances are constant.

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-6}{%
\section*{Next Week}\label{next-week-6}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  Inequalities
\end{itemize}

\includegraphics{./assets/img/markov.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{section-8}{%
\chapter*{Section 8}\label{section-8}}
\addcontentsline{toc}{chapter}{Section 8}

\markboth{Section 8}{Section 8}

Last Updated: 10 Nov 2023

Date: 10 Nov 2023

\hypertarget{this-week}{%
\section*{This Week}\label{this-week}}
\addcontentsline{toc}{section}{This Week}

\markright{This Week}

In this section, we will discuss:

\begin{itemize}
\tightlist
\item
  Inequality
\end{itemize}

\hypertarget{recap}{%
\section*{Recap}\label{recap}}
\addcontentsline{toc}{section}{Recap}

\markright{Recap}

Here are some important inequality recap!

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Cauchy-Schwartz: \({|E(XY)| \leq \sqrt{ E(X^2)E(Y^2) }}.\)
\item
  Monotonicity (of expectation): \(E(Y_1) \leq E(Y_2)\) for
  \(Y_1\leq Y_2.\)
\item
  Markov: for any \(a>0, {P(|Y|\geq a ) \leq \frac{E|Y|}{a} }.\)
\item
  Chebyshev: for any \(Y\sim [\mu,\sigma^2 (<\infty)]\) and
  \(\epsilon>0, {P(|Y-\mu| \geq \epsilon ) \leq \frac{\sigma^2}{\epsilon^2}}.\)
\item
  Chernoff: for any \(Y\) with MGF \(M(\cdot)\) and any \(a>0,t>0\)
  which \(M(t)\) exists, we have \(P(Y\geq a ) \leq e^{-at}M(t).\)
\item
  Concentration (Hoeffding): Let \(Y_1,Y_2,\cdots,Y_n\) be bounded,
  independent and \(\epsilon>0.\) If \(|Y_j|\leq c\) for each \(j\) then
  \(P( |\bar{Y_n} - E\bar{Y_n}|>\epsilon ) \leq 2 e^{-n\epsilon^2 /(2c^2)}.\)
  If \(a_j \leq Y_j \leq b_j\), then
  \(P( |\bar{Y_n} - E\bar{Y_n}|>\epsilon ) \leq 2 e^{-2n^2\epsilon^2 /(\sum (b_j-a_j)^2)}.\)
\item
  Convexity (Jensen): \(E[g(Y)] \geq g(EY)\) for \(g\) convex
  (\(g''(x) \geq 0.\))
\item
  Contraction: for any \(r\geq 1\), \(\| E(Y|X)\| _r \leq \|Y\|_r.\)
\item
  Correlation inequality: for \(g,h\) increasing functions
  \(Cor(g(Y),h(Y)) \geq 0.\)
\item
  Mills inequality: let \(Z \sim \N(0,1)\), then
  \(P(Z > t ) \leq \frac{\varphi(t)}{t}\) for all \(t>0.\)
\item
  Minkowski: for \(1\leq r <\infty\),
  \(\|X+Y\|_r \leq \|X\|_r + \|Y\|_r.\)
\item
  Monotonicity of norms: for any \(1 \leq r \leq s <\infty\),\$ \{
  \textbar Y\textbar\_r \leq \textbar Y\textbar\_s\}.\$
\item
  Conjugate Norms (Holder): for
  \(\frac{1}{r} + \frac{1}{s}= 1, {\|XY\|_1 \leq \|X\|_r \|Y\|_s}.\)
\item
  KL Divergence: \(D(f,g) = E_f \log \frac{f(X)}{g(X)}.\)
\item
  AM-GM-HM inequality: \(AM \geq GM \geq HM\), where AM stands for
  Arithmetic Means, GM stands for Geometric Means, and HM stands for
  Harmonic Means.
\end{enumerate}

\hypertarget{section-discussion-questions-6}{%
\section*{Section Discussion
Questions}\label{section-discussion-questions-6}}
\addcontentsline{toc}{section}{Section Discussion Questions}

\markright{Section Discussion Questions}

\hypertarget{section-problem-1-6}{%
\subsection*{✏️ Section Problem 1}\label{section-problem-1-6}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 1}

Let \(\mathrm{X}\) be a non-negative random variable with finite
variance, and let \(0 \leq \theta \leq 1\). (a) Prove that \[
\mathbb{P}(X>\theta E(X)) \geq(1-\theta)^2 \frac{E(X)^2}{E\left(X^2\right)} .
\]

Hint: Write
\(E(X)=E\left(X \mathbf{1}_{X \leq \theta E(X)}\right)+E\left(X \mathbf{1}_{X>\theta E(X)}\right)\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  The above inequality can actually be improved. Show that \[
  \mathbb{P}(X>\theta E(X)) \geq \frac{(1-\theta)^2 E(X)^2}{\operatorname{Var}(X)+(1-\theta)^2 E(X)^2}
  \] and confirm that this inequality is strictly stronger lower bound
  than the one in part (a). Denoting \(E(X)=\mu\) and
  \(\operatorname{Var}(X)=\sigma^2\), conclude that \[
  P(X>\mu-\theta \sigma) \geq \frac{\theta^2}{1+\theta^2}
  \] for \(0 \leq \mu-\theta \sigma \leq \mu\).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Following the hint, we write \[
  E(X)=E\left(X \mathbf{1}_{X \leq \theta E(X)}\right)+E\left(X \mathbf{1}_{X>\theta E(X)}\right) .
  \]
\end{enumerate}

The first addend \[
E\left(X 1_{X \leq \theta E(X)}\right) \leq \theta E[X]
\] while the second addend \[
E\left(X 1_{X>\theta E(X)}\right) \leq E\left[X^2\right]^{1 / 2} \mathbb{P}(X>\theta E[X])^{1 / 2}
\] by the Cauchy-Schwarz inequality. The desired inequality then
follows.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  By the Cauchy-Schwarz inequality; \[
  E(X-\theta E[X]) \leq E\left((X-\theta E[X]) 1_{X>\theta E(X)}\right) \leq E\left[(X-\theta E[X])^2\right]^{1 / 2} \mathbb{P}(X>\theta E[X])^{1 / 2}
  \] which, after rearranging, implies that \[
  \mathbb{P}(X>\theta E[X]) \geq \frac{(1-\theta)^2 E[X]^2}{E\left[(X-\theta E[X])^2\right]}=\frac{(1-\theta)^2 E[X]^2}{\operatorname{Var}(X)+(1-\theta)^2 E[X]^2} .
  \]
\end{enumerate}

The lower bound in part (a) can be rewritten as \[
\frac{(1-\theta)^2 E[X]^2}{\operatorname{Var}(X)+E[X]^2}
\] which is strictly smaller than \[
\frac{(1-\theta)^2 E[X]^2}{\operatorname{Var}(X)+(1-\theta)^2 E[X]^2}
\] provided that \(E[X]^2>0\) and \(0<\theta \leq 1\). The rest follows
by considering the substitution \(\theta=1-\tilde{\theta} \sigma / \mu\)
for \(0 \leq \mu-\tilde{\theta} \sigma \leq \mu\).

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-2-9.8}{%
\subsection*{✏️ Section Problem 2 (9.8)}\label{section-problem-2-9.8}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 2 (9.8)}

(Bound on sums of third absolute moments) Let \(X_{1} ,\dotsc ,X_{n}\)
be r.v.s with finite fourth moments. By using Cauchy-Schwarz, show that
the following inequality holds: \begin{equation*}
\sum _{j=1}^{n} E(|X_{j} |^{3} )\leq \sqrt{\left(\sum _{j=1}^{n} E(X_{j}^{2} )\right)\left(\sum _{j=1}^{n} E(X_{j}^{4} )\right)} .
\end{equation*} Hint: consider \(X_{J}\), where \(J\) is a random index
supported on \(\{1,\dotsc ,n\}\).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

Let \(\displaystyle Y=X_{J}\), where \(J\) is a random index supported
on \(\{1,\dotsc ,n\}\). By LOTP, \begin{equation}
E\left( |Y|^{\ell }\right) =\frac{1}{n}\sum _{j=1}^{n} E\left( |X_{j} |^{\ell }\right)
\end{equation} for \(\displaystyle \ell =2,3,4\). By Cauchy-Schwarz,
\begin{equation*}
E\left( |Y|^{3}\right) \leq \sqrt{E\left( |Y|^{2}\right) E\left( |Y|^{4}\right)} .
\end{equation*} So from (1), we get the inequality \begin{equation*}
\frac{1}{n}\sum _{j=1}^{n} E\left( |X_{j} |^{3}\right) \leq \sqrt{\left(\frac{1}{n}\sum _{j=1}^{n} E\left( |X_{j} |^{2}\right)\right)\left(\frac{1}{n}\sum _{j=1}^{n} E\left( |X_{j} |^{4}\right)\right)} .
\end{equation*} Removing the unnecessary absolute values on the left and
multiplying both sides by \(\displaystyle n\), we get the desired
inequality.

\end{minipage}%
\end{tcolorbox}

\hypertarget{section-problem-3-5}{%
\subsection*{✏️ Section Problem 3}\label{section-problem-3-5}}
\addcontentsline{toc}{subsection}{✏️ Section Problem 3}

Let \(X_1,X_2,\cdots\) be independent with mean 0 and
\(\sigma^2_i = \E\left(X_i^2\right) < \infty\) and define partial sums
\(S_k = X_1 + X_2 + \cdots + X_k.\) Then \begin{equation}
P\left( \max_{1\leq k \leq n} |S_k| \geq \epsilon \right) \leq \frac{\E\left(S_n^2\right)}{\epsilon^2}. 
\end{equation}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, left=2mm, colback=white, arc=.35mm, bottomrule=.15mm, rightrule=.15mm, leftrule=.75mm, breakable, opacityback=0, toprule=.15mm]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-tip-color}{\faLightbulb}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\textbf{Solution}\vspace{2mm}

\hypertarget{solution-55}{%
\section*{Solution}\label{solution-55}}
\addcontentsline{toc}{section}{Solution}

\markright{Solution}

Let \(A_k = \{|S_k| \geq \epsilon, |S_i| < \epsilon \ \forall i < k\}\).
This is the sets of events consisting of \(X_1,\cdots, X_n\) such that
\(|S_k|\) is the first partial sum with absolute value greater than
\(\epsilon.\) We can see that \(A_k\)'s are disjoint and the union
\[\cup_{i=1}^n A_i = \{\exists k\in\{1,...,n\} \ s.t. \  |S_k| \geq \epsilon\} = \{\max_{1\leq k \leq n } |S_k| \geq \epsilon\}.\]

We must have
\[ \E\left(S_n^2\right) \geq \E\left(S_n^2 \mathbb{I} \left(\cup_{k=1}^n A_k\right)\right) = \sum_{k=1}^{n}\E\left(S_n^2 \mathbb{I}(A_k)\right),\]
because \(A_k\)'s are disjoint and indicator function is always
\(\leq 1\). For each \(k\) we must have
\(S_n^2 = S_k^2 + 2(S_n-S_k)S_k + (S_n-S_k)^2\) and therefore \[
E\left(S_n^2 \mathbb{I}(A_k)\right) = E\left(S_k^2 \mathbb{I}(A_k)\right) + E\left( (S_n-S_k)^2 \mathbb{I}(A_k) \right)+ 2 E\left((S_n-S_k)S_k \mathbb{I}(A_k)\right) .\]
We know that \(E\left( (S_n-S_k)^2 \mathbb{I}(A_k) \right) \geq 0\) and
since \(X_i\)'s are iid and \(\mathbb{I}(A_k)\) only depends on
\(X_1,\cdots,X_k\) we must have
\[E\left((S_n-S_k)S_k \mathbb{I}(A_k)\right)  = E\left[S_n-S_k\right] E\left[S_k\mathbb{I}(A_k)\right]= 0.\]

More importantly
\[E(S_k^2\mathbb{I}(A_k)) = \mathbb{P}(A_k) E\left[S_k^2|A_k\right] \geq \mathbb{P}(A_k)\epsilon^2.\]
So
\[ \E\left(S_n^2 \mathbb{I}(A_k)\right) \geq \mathbb{P}(A_k) \epsilon^2 \quad \forall k.\]
Finally
\[ E(S_n^2) \geq \sum_{k=1}^n \mathbb{P}(A_k)\epsilon^2 = \epsilon^2 \mathbb{P}\left(\cup_{k=1}^n A_k\right) = \epsilon^2 \mathbb{P}\left(\max_{1\leq k \leq n} |S_k| \geq \epsilon\right).\]

\end{minipage}%
\end{tcolorbox}

\hypertarget{next-week-7}{%
\section*{Next Week}\label{next-week-7}}
\addcontentsline{toc}{section}{Next Week}

\markright{Next Week}

Next week, we will discuss:

\begin{itemize}
\tightlist
\item
  Convergence
\end{itemize}

\includegraphics{./assets/img/convergence.jpeg}

Feel free to upload the pencil problem you wish to be discussed next
week \href{https://forms.gle/RBmMNYJp4u3qD5W79}{here}.

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)

\bookmarksetup{startatroot}

\hypertarget{quotes}{%
\chapter*{Quotes}\label{quotes}}
\addcontentsline{toc}{chapter}{Quotes}

\markboth{Quotes}{Quotes}

\hypertarget{quotes-1}{%
\section*{Quotes}\label{quotes-1}}
\addcontentsline{toc}{section}{Quotes}

\markright{Quotes}

\begin{quote}
How will our participation grade be measured? \emph{Joe}: Using measure
theory.
\end{quote}

\begin{quote}
Measure theory is just about measuring things - \emph{Jun Liu}, Spring
2023
\end{quote}

\begin{quote}
What I don't like about measure theory is that you have to say ``almost
everywhere'' almost everywhere. - \emph{Kurt Friedrichs}
\end{quote}

\hypertarget{upload-your-meme-quotes}{%
\section*{Upload your meme / quotes}\label{upload-your-meme-quotes}}
\addcontentsline{toc}{section}{Upload your meme / quotes}

\markright{Upload your meme / quotes}

If you would like to contribute to this meme + quotes page, please
upload it at this \href{https://forms.gle/RBmMNYJp4u3qD5W79}{form}

Note that a verified email address is needed in the GForm so we don't
get scammy input! :)

\(\,\)



\end{document}
